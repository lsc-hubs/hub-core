[
  {
    "objectID": "docs/developer/tutorial-data-management/slides/index.html",
    "href": "docs/developer/tutorial-data-management/slides/index.html",
    "title": "Slides training on spatial data flows",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 15, 2023\n\n\n1.1 Training Overview\n\n\nPaul van Genuchten\n\n\n\n\nSep 15, 2023\n\n\n1.2 Findable\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n1.3 Tools\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n2.1 Collect and Publish metadata\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n3.1 OGC data services\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n3.2 Service Quality\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n4.1 Soil Profile Datamanagement\n\n\nPaul van Genuchten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#findable",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#findable",
    "title": "1.2 Findable",
    "section": "Findable",
    "text": "Findable\n\nThe first step in (re)using data is to find them.\nMetadata and data should be easy to find for both humans and computers.\nMachine-readable metadata are essential for automatic discovery of datasets and services"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistent-identification",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistent-identification",
    "title": "1.2 Findable",
    "section": "Persistent identification",
    "text": "Persistent identification\nPersistent identification, for continued findability\n\nConsider that a proper id can outlive a project (or organisation)\nChoice of domain and path (owned, authoritative, neutral, prevent names)\nSet up an identification proxy (doi.org/w3id.org)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#metadata",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#metadata",
    "title": "1.2 Findable",
    "section": "Metadata",
    "text": "Metadata\n\nMetadata describes title, abstract, author of a resource\nFacilitate findability and understand if a resource is relevant\nCan organize resources in groups by tagging"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#standards-for-metadata-exchange",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#standards-for-metadata-exchange",
    "title": "1.2 Findable",
    "section": "Standards for metadata exchange",
    "text": "Standards for metadata exchange\n\nFacilitate the exchange of resources between communities\nProtocols to exchange metadata:\n\n\n\n\nCommunity\nStandard\nProtocol\n\n\n\n\nOpen data/Sematic web\nDCAT\nSPARQL\n\n\nScience\nDatacite\nOAI-PMH\n\n\nGeospatial\niso19115\nCSW\n\n\nEarth observation\nSTAC\nSTAC\n\n\nSearch engines\nSchema.org\njson-ld/microdata\n\n\nEcology\nEML"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#catalogue",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#catalogue",
    "title": "1.2 Findable",
    "section": "Catalogue",
    "text": "Catalogue\n\nRecords are brought into a catalogue, where they can be searched and assessed\nCatalogues can exchange records to increase discoverability\nCatalogues can cross borders between communities by transforming metadata to relevant community standards and protocols"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#search-engines",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#search-engines",
    "title": "1.2 Findable",
    "section": "Search engines",
    "text": "Search engines\n\nSearch engines crawl the content of catalogues\nIf a catalogue supports schema.org annotations, the content can also be extracted in a structured way\nExample"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistence",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistence",
    "title": "1.2 Findable",
    "section": "Persistence",
    "text": "Persistence\n\nMove the resource to a shared environment (backup)\nConsider a URL strategy\nUse a facade identifier (DOI)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#data-lifecycle",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#data-lifecycle",
    "title": "1.2 Findable",
    "section": "Data lifecycle",
    "text": "Data lifecycle\n\nConsider upfront when to remove a resource (10 yrs?)\nWhat happens to the URI of a resource which is archived?\nMetadata should stay available, even if the data are no longer"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#repository-software",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#repository-software",
    "title": "1.2 Findable",
    "section": "Repository software",
    "text": "Repository software\n\nWebdav (or webserver software)\nZenodo, Dataverse\nDocument Management Systems (DMS)\nCloud storage (google drive, dropbox, Amazon, Sharepoint)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#testing-tools",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#testing-tools",
    "title": "1.2 Findable",
    "section": "testing tools",
    "text": "testing tools\n\nAutomated link checking\nUsage logs filter on status 404 & referer\nGoogle Search Console notifies broken links"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#universal-formats",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#universal-formats",
    "title": "1.2 Findable",
    "section": "Universal formats",
    "text": "Universal formats\nFacilitates accessing a resource with commonly available tooling, or be refactored if a software is abandoned\n\nProprietary vs Open (eg. ecw vs tiff)\nDe facto vs Formalised (eg. YAML vs XML)\nBinary vs text based (eg. shapefile vs GeoJSON)\nCloud optimised vs Cloud native vs traditional (eg. COG vs GeoZarr vs GeoTiff)\nEmbedded metadata"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#adopt-common-vocabularies",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#adopt-common-vocabularies",
    "title": "1.2 Findable",
    "section": "Adopt common vocabularies",
    "text": "Adopt common vocabularies\nAdopting a standardised model enables aggregation of data.\n\nRelational models\nUML/GML models\nSemantic web ontologies"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#relevant-vocabularies",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#relevant-vocabularies",
    "title": "1.2 Findable",
    "section": "Relevant vocabularies",
    "text": "Relevant vocabularies\n\nISO28258 / INSPIRE / GLOSIS Web Ontology\nAgrovoc\nWRB / FAO Soil Classification Guidelines"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Land Soil Crop Hub",
    "section": "",
    "text": "This hub facilitates effective access to Land Soil and Crop information data, making existing resources better findable, accessible, interoperable and reusable (FAIR).\nThe LSC hub supports improved decision-making for climate-smart agriculture at national, regional and local levels. The focus is on two use cases: soil fertility management and soil water conservation."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Land Soil Crop Hub",
    "section": "Data",
    "text": "Data\nThe LSC hub offers a diverse range of datasets, maps and documents, including observational data from field studies (e.g., soil and water samples, crop yields), aerial and space observations, administrative boundaries, economic data (price development), predictive model outputs (soil, weather, yields), and population statistics.\n\nDatasets\nSearch datasets or browse by category.\n\n\n\n\n\n\n\n\n\n\n Land\n\n Soil\n\n Crop\n\n Climate\n\n Water\n\n\n\n\n\n\n\n\nMaps\nAnnual cropland extent map for Central Africa produced by Digital Earth.\n\n\n\n\n\n\nOpen mapviewer\n\n\n\n\n\nDocuments\nRead more about the various API’s which are available for this service.\n\n\n\n\n\n\nView documents"
  },
  {
    "objectID": "index.html#information-services",
    "href": "index.html#information-services",
    "title": "Land Soil Crop Hub",
    "section": "Information services",
    "text": "Information services\nInformation services provide dedicated information derived from relevant data to a targeted audience.\nProcessing raw data in many cases requires expert knowledge. To bring the value of the data to a wider audience, it is of interest to set up information services derived from that data to targeted audiences.\nBrowse services Read more"
  },
  {
    "objectID": "index.html#predictive-modeling",
    "href": "index.html#predictive-modeling",
    "title": "Land Soil Crop Hub",
    "section": "Predictive modeling",
    "text": "Predictive modeling\nPredictive modeling in the LSC hub focuses on Soil Fertility and Soil Water Conservation and is a data-driven analytical approach that involves the use of statistical or machine learning techniques to create models that can make predictions about future events or outcomes based on historical data.\nBrowse models Read more"
  },
  {
    "objectID": "index.html#user-stories",
    "href": "index.html#user-stories",
    "title": "Land Soil Crop Hub",
    "section": "User stories",
    "text": "User stories\nThe two cases below are descriptions of key applications for which the LSC hub can be used, including the main stakeholders, the main issues and the models through which ‘LSC data’ is converted into information services that support informed decision making.\n\n\n\n\nSoil Fertility Management\nCurrent fertilizer and soil recommendations lack local context, leading to soil health decline and lower productivity. Integrated Soil Fertility Management can improve practices, boost yields, and provide climate benefits. This case aims to deliver better agronomic advice to farmers via agricultural extension services or directly, using existing data and tools.\n\nExplore this use case\n\n\n\n\n\nSoil Water Conservation\nSoil erosion threatens sustainability, climate, and food security in hilly regions of Ethiopia, Kenya, and Rwanda. Current land practices neglect local factors and erosion risks. The goal is to inform stakeholders and promote sustainable land practices for LDN, providing catchment managers and farmers with relevant information through the LSC-hub.\n\nExplore this use case"
  },
  {
    "objectID": "index.html#hub-community",
    "href": "index.html#hub-community",
    "title": "Land Soil Crop Hub",
    "section": "Hub community",
    "text": "Hub community\nThe heart of the knowledge hub. Here, you’ll find like-minded participants who share a passion for learning and knowledge exchange.\nJoin our diverse community of learners, experts, and enthusiasts to engage in discussions, share insights, and collaborate on soil fertility and soil water conservation topics. Explore, connect, contribute and make the hub grow.\n\nVisit community"
  },
  {
    "objectID": "index.html#join-our-newsletter",
    "href": "index.html#join-our-newsletter",
    "title": "Land Soil Crop Hub",
    "section": "Join our newsletter",
    "text": "Join our newsletter\nSubscribe to our newsletter and be the first to receive the latest data updates and community news about the Kenya Land Soil and Crop hub.\n\nSubscribe now"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "LSC Hubs documentation",
    "section": "",
    "text": "Documentation of this hub has been split up in 3 groups, for users, for administrators and for developers.\n\n\n\nUser documentation\nAdministrator documentation\nDeveloper documentation"
  },
  {
    "objectID": "docs/index.html#contents",
    "href": "docs/index.html#contents",
    "title": "LSC Hubs documentation",
    "section": "",
    "text": "User documentation\nAdministrator documentation\nDeveloper documentation"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html",
    "title": "Advanced options",
    "section": "",
    "text": "Various extensions are possible to tailor the system to your organisation needs.\n\n\nThe pycsw configuration file has a number of configuration options to tailer the title, abstract, etc, of the application. But you can also tailor the jinja2 templates. We published a tailored set of templates as a pycsw skin on github.\npycsw allows to extend the internal metadata schema via mapping configuration or map its model to an existing database. This configuration can facilitate additional queryables or extra elements on metadata records, etc.\n\n\n\nTerriaJS is a modern web gis application, which includes a widget to query a catalogue. From the catalogue search results the data can be added to the TerriaJS map.\nThe main docker image definition can be used to build and run terriaJS locally.\ngit clone https://github.com/TerriaJS/TerriaMap\ncd TerriaMap\ndocker build -t local/terria .\ndocker run -p 3001:3001 local/terria\nVisit http://localhost:3001 to see TerriaJS in action.\n\n\n\nWhen using pygeometa to render a iso19139 document, you can use a tailered output schema, to match organisation needs (for example use a hardcoded publisher section).\nMCF allows to add any additional properties not listed on the json schema, which is a very easy way to extend the schema. However note that MCF validation may fail.\nGeoDataCrawler internally uses pygeometa to manage mcf, a location of a extended schema is one of the optional parameters to GeoDataCrawler.\nMDME does not use the mcf json schema as-is, because some of the mcf json schema elements are not supported by MDME. In case you extend the MCF schema and aim to use MDME, you need to also extend the MDME schema likewise.\n\n\n\n\nIn some cases it is relevant as a national or regional coordinator to publish a (extended) codelist which provides concepts which are only relevant within your area. users in your area can then easily link to the concepts in that vocabulary and benefit from the regional interoperability.\nThe SKOS ontology is a commonly accepted mechanism to create vocabularies. It allows to link concepts to similar, wider or narrower concepts. When creating a new vocabulary you typically start with a list of concepts in Excel.\nThe Glosis web ontology contains a transformer script which convert a csv (exported from Excel) file to a skos document, alternatively you can use a tool such as skos play. The skos file can best be hosted on a git repository. Anybody can access it, and provide improvement suggestions.\nThe python vocview library is able to render a skos file to a nice web layout which is easy to browse and search through by humans. See a live implementation at tern."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#pycsw",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#pycsw",
    "title": "Advanced options",
    "section": "",
    "text": "The pycsw configuration file has a number of configuration options to tailer the title, abstract, etc, of the application. But you can also tailor the jinja2 templates. We published a tailored set of templates as a pycsw skin on github.\npycsw allows to extend the internal metadata schema via mapping configuration or map its model to an existing database. This configuration can facilitate additional queryables or extra elements on metadata records, etc."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#terriajs",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#terriajs",
    "title": "Advanced options",
    "section": "",
    "text": "TerriaJS is a modern web gis application, which includes a widget to query a catalogue. From the catalogue search results the data can be added to the TerriaJS map.\nThe main docker image definition can be used to build and run terriaJS locally.\ngit clone https://github.com/TerriaJS/TerriaMap\ncd TerriaMap\ndocker build -t local/terria .\ndocker run -p 3001:3001 local/terria\nVisit http://localhost:3001 to see TerriaJS in action."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#tailored-metadata-schema-in-mcf-pygeometa",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#tailored-metadata-schema-in-mcf-pygeometa",
    "title": "Advanced options",
    "section": "",
    "text": "When using pygeometa to render a iso19139 document, you can use a tailered output schema, to match organisation needs (for example use a hardcoded publisher section).\nMCF allows to add any additional properties not listed on the json schema, which is a very easy way to extend the schema. However note that MCF validation may fail.\nGeoDataCrawler internally uses pygeometa to manage mcf, a location of a extended schema is one of the optional parameters to GeoDataCrawler.\nMDME does not use the mcf json schema as-is, because some of the mcf json schema elements are not supported by MDME. In case you extend the MCF schema and aim to use MDME, you need to also extend the MDME schema likewise."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#publishing-extended-vocabularies",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#publishing-extended-vocabularies",
    "title": "Advanced options",
    "section": "",
    "text": "In some cases it is relevant as a national or regional coordinator to publish a (extended) codelist which provides concepts which are only relevant within your area. users in your area can then easily link to the concepts in that vocabulary and benefit from the regional interoperability.\nThe SKOS ontology is a commonly accepted mechanism to create vocabularies. It allows to link concepts to similar, wider or narrower concepts. When creating a new vocabulary you typically start with a list of concepts in Excel.\nThe Glosis web ontology contains a transformer script which convert a csv (exported from Excel) file to a skos document, alternatively you can use a tool such as skos play. The skos file can best be hosted on a git repository. Anybody can access it, and provide improvement suggestions.\nThe python vocview library is able to render a skos file to a nice web layout which is easy to browse and search through by humans. See a live implementation at tern."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "For spatial datasets it is of interest to share them via conveniance API’s, so the datasets can be downloaded in parts or easily be visualised in common tools such as QGIS, OpenLayers & Leaflet. The standards of the Open Geospatial Consortium are most relevant. These API’s can give direct access to subsets or map vizualisations of a dataset.\nIn this paragraph you are introduced to various standardised API’s, after which we introduce you to an approach to publish your datasets, which builds on the data management approach introduced in the previous paragraphs.\n\n\n\nOpen Geospatial Consortium has a long history of standardisation efforts. Standardised mapping api’s, such as Web Map Service (WMS), Web Feature service (WFS) and Web Coverage Service (WCS), originate from the beginning of this century. In recent years several challenges have been identified around these standards, which led to a series of Spatial data on the web best practices. OGC then initiated a new generation of standards based on these best practices.\nAn overview of both generations:\n\n\n\n\n\n\n\n\nType\nOWS\nOGC-API\n\n\n\n\nMap visualisation\nWeb Map service (WMS), Web Maps Tile Service (WMTS)\nOGCAPI:Maps, OGCAPI:Tiles\n\n\nVector access\nWeb Feature Service (WFS)\nOGCAPI:Features\n\n\nGrid access\nWeb Coverage Service (WCS)\nOGCAPI:Coverages\n\n\nSensor Observations\nSensor Observation Service (SOS)\nSensortthings\n\n\nProcesses\nWeb Processing Service (WPS)\nOGCAPI:Processes\n\n\nCatalogues\nCatalogue Service for the web (CSW)\nOGCAPI:Records\n\n\n\nNotice that most of the mapping software supports the standards of both generations. However, due to the recent introduction, expect incidental glitches in the implementations of recent OGC API’s.\n\n\n\n\nMapserver is server software which is able to expose datasets through various API’s. Examples of similar software are QGIS server, ArcGIS Server, Geoserver and pygeoapi.\nWe’ve selected mapserver for this training, because of its robustness and low resource consumption. Mapserver is configured using a configuration file, a mapfile. The mapfile defines metadata for the dataset and how users interact with the dataset, mainly the color scheme (legend) to draw a map of the dataset.\nVarious tools exist to write these configuration files, such as Mapserver studio, GeoStyler, QGIS Bridge, up to a visual studio plugin to edit mapfiles.\nThe GeoDataCrawler, introduced in a previous paragraph, also has an option to generate mapfiles. A big advantage of this approach is the integration with existing metadata. Many publication workflows require to add similar metadata at various locations, a risk for disambiguity. GeoDataCrawler will, during mapfile generation, use the existing metaddata, but also update the metadata so it includes a link to the mapserver service endpoint. This step enables a typical workflow of:\n\nUser finds a dataset in a catalogue\nThen opens the dataset via the linked service\n\nAs well as vice versa; from a mapping application, access the metadata describing a dataset.\n\n\n\n\n\nNavigate with shell to a folder with data files.\nVerify if mcf’s are available for the files, if not, create initial metadata with crawl-metadata --mode=init --dir=.\nAdd a index.yml file to the folder. This metadata is introduced in the mapfile to identify the service.\n\nmcf:\n   version: 1.0\nidentification:\n    title: My new mapservice\n    abstract: A map service for data about ...\ncontact:\n  pointOfContact:\n    organization: ISRIC\n    email: info@isric.org\n    url: https://www.isric.org\n\nGenerate the mapfile\n\ncrawl-maps --dir=./\n\nIndex.yml may include a “robot” property, to guide the crawler in how to process the folder. This section can be used to add specific crawling behaviour.\n\nmcf:\n    version: 1.0\nrobot:\n    skip-subfolders: True # indicates the crawler not to proceed in subfolders\nYou can test this mapfile locally if you have mapserver installed. On windows, consider using conda or ms4w.\nconda install -c conda-forge mapserver\nMapserver includes a map2img utility, which enables to render a map image from any mapfile.\nmap2img -m=./mymap.map -o=test.png\n\n\n\n\nFor this exersize we’re using a mapserver image available from Docker hub (we’re using master awaiting the 8.2 release).\ndocker pull camptocamp/mapserver:master  \nFirst create a config file, which we’ll mount as a volume into the container. On this config file we list all the mapfiles we aim to publish on our service. Download the default config file. Open the file and unescape and populate the maps section:\nMAPS\n     \"data\" \"/srv/data/data.map\"\nEND\nAlso unescape the OGCAPI templates section\nOGCAPI_HTML_TEMPLATE_DIRECTORY \"/usr/local/share/mapserver/ogcapi/templates/html-bootstrap4/\"\nNotice the path /srv/data replacing the local path, /srv/data is the folder we use within the container (mounted).\nIn the next statement we mount the data folder, including the config file and indicate on which port and with which config file the container will run:\ndocker run \\\n    -p 80:80 \\\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf \\\n    -v $(pwd):/srv/data  \\\n    camptocamp/mapserver:master \nCheck http://localhost/data/ogcapi in your browser. If all has been set up fine it should show the OGCAPI homepage of the service. If not, check the container logs to evaluate any errors.\nYou can also try the url in QGIS. Add a WMS layer, of service http://localhost/data?request=GetCapabilities&service=WMS. Notice the links to metadata when you open GetCapabilities in a browser.\n[!NOTE] In recent years browsers have become more strict, to prevent abuse. For that reason it is important to carefully consider common connectivity aspects, when setting up a new service. Websites running at https can only embed content from other https services, so using https is relevant. CORS and CORB can limit access to embedded resources from remote servers. Using proper CORS headers and Content type identification is relevant to prevent CORS and CORB errors.\n\n\n\n\nIn this paragraph the standards of Open Geospatial Consortium have been introduced and how you can publish your data according to these standards using Mapserver. In the next section we’ll look at measuring service quality."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#standardised-data-apis",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#standardised-data-apis",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "Open Geospatial Consortium has a long history of standardisation efforts. Standardised mapping api’s, such as Web Map Service (WMS), Web Feature service (WFS) and Web Coverage Service (WCS), originate from the beginning of this century. In recent years several challenges have been identified around these standards, which led to a series of Spatial data on the web best practices. OGC then initiated a new generation of standards based on these best practices.\nAn overview of both generations:\n\n\n\n\n\n\n\n\nType\nOWS\nOGC-API\n\n\n\n\nMap visualisation\nWeb Map service (WMS), Web Maps Tile Service (WMTS)\nOGCAPI:Maps, OGCAPI:Tiles\n\n\nVector access\nWeb Feature Service (WFS)\nOGCAPI:Features\n\n\nGrid access\nWeb Coverage Service (WCS)\nOGCAPI:Coverages\n\n\nSensor Observations\nSensor Observation Service (SOS)\nSensortthings\n\n\nProcesses\nWeb Processing Service (WPS)\nOGCAPI:Processes\n\n\nCatalogues\nCatalogue Service for the web (CSW)\nOGCAPI:Records\n\n\n\nNotice that most of the mapping software supports the standards of both generations. However, due to the recent introduction, expect incidental glitches in the implementations of recent OGC API’s."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setting-up-an-api",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setting-up-an-api",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "Mapserver is server software which is able to expose datasets through various API’s. Examples of similar software are QGIS server, ArcGIS Server, Geoserver and pygeoapi.\nWe’ve selected mapserver for this training, because of its robustness and low resource consumption. Mapserver is configured using a configuration file, a mapfile. The mapfile defines metadata for the dataset and how users interact with the dataset, mainly the color scheme (legend) to draw a map of the dataset.\nVarious tools exist to write these configuration files, such as Mapserver studio, GeoStyler, QGIS Bridge, up to a visual studio plugin to edit mapfiles.\nThe GeoDataCrawler, introduced in a previous paragraph, also has an option to generate mapfiles. A big advantage of this approach is the integration with existing metadata. Many publication workflows require to add similar metadata at various locations, a risk for disambiguity. GeoDataCrawler will, during mapfile generation, use the existing metaddata, but also update the metadata so it includes a link to the mapserver service endpoint. This step enables a typical workflow of:\n\nUser finds a dataset in a catalogue\nThen opens the dataset via the linked service\n\nAs well as vice versa; from a mapping application, access the metadata describing a dataset."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#mapfile-creation-exersize",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#mapfile-creation-exersize",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "Navigate with shell to a folder with data files.\nVerify if mcf’s are available for the files, if not, create initial metadata with crawl-metadata --mode=init --dir=.\nAdd a index.yml file to the folder. This metadata is introduced in the mapfile to identify the service.\n\nmcf:\n   version: 1.0\nidentification:\n    title: My new mapservice\n    abstract: A map service for data about ...\ncontact:\n  pointOfContact:\n    organization: ISRIC\n    email: info@isric.org\n    url: https://www.isric.org\n\nGenerate the mapfile\n\ncrawl-maps --dir=./\n\nIndex.yml may include a “robot” property, to guide the crawler in how to process the folder. This section can be used to add specific crawling behaviour.\n\nmcf:\n    version: 1.0\nrobot:\n    skip-subfolders: True # indicates the crawler not to proceed in subfolders\nYou can test this mapfile locally if you have mapserver installed. On windows, consider using conda or ms4w.\nconda install -c conda-forge mapserver\nMapserver includes a map2img utility, which enables to render a map image from any mapfile.\nmap2img -m=./mymap.map -o=test.png"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setup-mapserver-via-docker-exersize",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setup-mapserver-via-docker-exersize",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "For this exersize we’re using a mapserver image available from Docker hub (we’re using master awaiting the 8.2 release).\ndocker pull camptocamp/mapserver:master  \nFirst create a config file, which we’ll mount as a volume into the container. On this config file we list all the mapfiles we aim to publish on our service. Download the default config file. Open the file and unescape and populate the maps section:\nMAPS\n     \"data\" \"/srv/data/data.map\"\nEND\nAlso unescape the OGCAPI templates section\nOGCAPI_HTML_TEMPLATE_DIRECTORY \"/usr/local/share/mapserver/ogcapi/templates/html-bootstrap4/\"\nNotice the path /srv/data replacing the local path, /srv/data is the folder we use within the container (mounted).\nIn the next statement we mount the data folder, including the config file and indicate on which port and with which config file the container will run:\ndocker run \\\n    -p 80:80 \\\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf \\\n    -v $(pwd):/srv/data  \\\n    camptocamp/mapserver:master \nCheck http://localhost/data/ogcapi in your browser. If all has been set up fine it should show the OGCAPI homepage of the service. If not, check the container logs to evaluate any errors.\nYou can also try the url in QGIS. Add a WMS layer, of service http://localhost/data?request=GetCapabilities&service=WMS. Notice the links to metadata when you open GetCapabilities in a browser.\n[!NOTE] In recent years browsers have become more strict, to prevent abuse. For that reason it is important to carefully consider common connectivity aspects, when setting up a new service. Websites running at https can only embed content from other https services, so using https is relevant. CORS and CORB can limit access to embedded resources from remote servers. Using proper CORS headers and Content type identification is relevant to prevent CORS and CORB errors."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#summary",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#summary",
    "title": "Providing conveniance API’s",
    "section": "",
    "text": "In this paragraph the standards of Open Geospatial Consortium have been introduced and how you can publish your data according to these standards using Mapserver. In the next section we’ll look at measuring service quality."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "This page introduces a number of generic Git functionalities and vendor add ons. Which can support communities in efficient co-creation of content. The page mainly focusses on the Continuous Integration & Deployment functionality, but contains many external links to introduce other aspects of Git. Considering the previous materials, a relevant ci-cd case is a set of tasks to run after a change to some of the mcf documents in a data repository, to validate the mcf’s and convert them to iso19139 and push them to a catalogue.\n\n\n\nIn its core GIT is a version management system traditionally used for maintaining software codes. In case you never worked with GIT before, have a look at this Git & Github explanation. Some users interact with Git via the command line (shell). However excellent Graphical User Interfaces exist to work with Git repositories, such as Github Desktop, a Git client within Visual Studio, TortoiseGit, Smartgit, and many others.\nThese days GIT based coding communities like Github, Gitlab, Bitbucket offer various services on top of Git to facilitate in co-creation of digital assets. Those services include authentication, issue management, release management, forks, pull requests and CI/CD. The types of digital assets maintained via GIT vary from software, deployment scripts, configuration files, documents, website content, metadata records up to actual datasets. Git is most effective with text based formats, which explains the popularity of formats like CSV, YAML, Markdown.\n\n\n\n\nContinuous Integration & Deployment describes a proces in which changes in software or configuration are automatically tested and deployed to a relevant environment. These processes are commonly facilited by GIT environments. With every commit to the Git repository an action is triggered which runs some tasks.\n\n\n\n\nThis exersize introduces the CI-CD topic by setting up a basic markdown website in Github Pages, maintained through Git. Markdown is a popular format to store text with annotations on Git.The site will be based on Quarto. Quarto is one of many platforms to generate a website from a markdown repository.\n\nCreate a new repository in your github account, for example ‘My first CMS’. Tick the ’’\nBefore we add any content create a branch ‘gh-pages’ on the repository, this branch will later contain the generated html sources of the website.\nCreate a file _quarto.yml into the new git repository, with this content:\n\nproject:\n  type: website\nwebsite:\n  title: \"hello world\"\n  navbar:\n    left:\n      - href: index.md\n        text: Home\n      - about.md\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nCreate file docs/index.md and docs/about.md. Start each file with a header:\n\n---\ntitle: Hello World\nauthor: Peter pan\ndate: 2023-11-11\n---\nAdd some markdown content to each page (under the header), for example:\n# Welcome\n\nWelcome to *my website*.\n\n- I hope you enjoy it.\n- Visit also my [about](./about.md) page.\n\nNow click on Actions in the github menu. Notice that Github has already set up a workflow to publish our content using jekyll, it should already be available at https://user.github.io/repo. We will not use this approach for now.\nRemove the existing workflow, generated by Github in Actions, Workflows, Remove\nFirst you need to allow the workflow-runner to make changes on the repository. For this, open Settings, Actions, General. Scroll down to Workflow permissions. Tick the Read and write permissions and click Save. If the option is grayed out, you first need to allow this feature in your organization.\nThen, from Actions, select New workflow, then set up a workflow yourself.\nOn the next page we will create a new workflow script, which is stored in the repository at /.github/workflows/main.yml.\n\nname: Docs Deploy\n\non:\n  push:\n    branches: \n      - main\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with: \n          tinytex: true \n          path: docs\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: docs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      \n\nSave the file, via actions you can follow the progress of the workflow at every push to the repository.\nOn the logs notice how a container is initialised, the source code is checked out, the mkdocs dependency is installed, the build is made and pushed to the gh-pages branch.\n\nNotice that the syntax to define workflows is different for every CI-CD platform, however they generally follow a similar pattern. For Github identify in the file above:\n\nIt defines at what events the workflow should trigger (in this case at push events).\na build job is triggered, which indicates a container image (runs-on) to run the job in, then triggers some steps.\nThe final step triggers a facility of quarto to publish its output to a github repository\n\nThe above setup is optimal for co-creating a documentation repository for your community. Users can visit the source code via the edit on github link and suggest improvements via issues of pull requests. Notice that this tutorial is also maintained as markdown in Git.\n\n\n\n\nFor this scenario we need a database in the cloud to host our records (which is reachable by github workflows). For the training we suggest to use a trial account at elephantsql.com.\n\nAt elephantsql, create a new account.\nThen create a new Instance of type Tiny (free).\nClick on the instance and notice the relevant connection string (URL) and password\nConnect your instance of pycsw to this database instance, by updating pycsw.cfg and following the instructions at Catalogue publication\nVerify in elephantsql dashboard if the records are correctly loaded (or install and configure pgadmin).\n\nWe will now publish our records from Github to our database.\n\nCreate a new repository on Github for the records\nMake sure git-scm (or a GUI tool like Git kraken, Gitlab) is intalled on your system (alternatively upload the files through the github web interface)\nClone (download) the repository to a local folder.\n\ngit clone https://github.com/username/records-repo.git\n\nCopy the mcf files, which have been generated in Catalogue publication, to a datasets folder in the cloned repository.\nCommit the files\n\ngit add -A && git commit -m \"Your Message\"\nBefore you can push your changes to Github, you need to set up authentication, generally 2 options are possible: - Using a personal access token - Or using SSH public key\ngit push origin main\nWe’ll now set up CI-CD to publish the records\n\nPlace the pycsw.cfg file in the root of the repositry (including the postgres database connection)\nCreate a new custom workflow file with this content:\n\nname: Records Deploy\n\non: \n  push:\n    paths:\n      - '**'\n\ndefaults:\n  run:\n    working-directory: .\n\njobs:\n  build:\n    name: Build and Deploy Records\n    runs-on: ubuntu-latest\n    steps:\n        - uses: actions/checkout@v3\n        - uses: actions/setup-python@v4\n          with:\n              python-version: 3.9\n        - name: Install dependencies\n          run: |\n            sudo add-apt-repository ppa:ubuntugis/ppa\n            sudo apt-get update\n            sudo apt-get install gdal-bin\n            sudo apt-get install libgdal-dev\n            ogrinfo --version\n            pip install GDAL==3.4.3\n            pip install geodatacrawler pycsw sqlalchemy\n        - name: Crawl metadata\n          run: |\n            export pgdc_webdav_url=http://localhost/collections/metadata:main/items\n            export pgdc_canonical_url=https://github.com/pvgenuchten/data-training/tree/main/datasets/\n            crawl-metadata --dir=./datasets --mode=export --dir-out=/tmp\n        - name: Publish records\n          run: |   \n            pycsw-admin.py delete-records --config=./pycsw.cfg -y\n            pycsw-admin.py load-records --config=./pycsw.cfg  --path=/tmp\n\nVerify that the records are loaded on pycsw (through postgres)\nChange or add some records to GIT, and verify if the changes are published (may take some time)\n\nNormally, one would not add a connection string to a database in a config file posted on Github. Instead Github offers secrets to capture this type of information.\n\n\n\n\nWhile users are browsing the catalogue (or this page), they may find irregularities in the content. They can flag this as an issue in the relevant Git repository. A nice feature is to add a link in the catalogue page which brings them back to the relevant mcf in the git repository. With proper authorisations they can instantly improve the record, or suggest an improvement via an issue or pull request.\n\n\n\n\nIn this section you learned about using actions in Github (CI/CD). In the next section we are diving into OGC Map Services using mapserver. Notice that you can use these CI/CD mechanisms to deploy or evaluate metadata and data services."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#git-content-versioning",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#git-content-versioning",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "In its core GIT is a version management system traditionally used for maintaining software codes. In case you never worked with GIT before, have a look at this Git & Github explanation. Some users interact with Git via the command line (shell). However excellent Graphical User Interfaces exist to work with Git repositories, such as Github Desktop, a Git client within Visual Studio, TortoiseGit, Smartgit, and many others.\nThese days GIT based coding communities like Github, Gitlab, Bitbucket offer various services on top of Git to facilitate in co-creation of digital assets. Those services include authentication, issue management, release management, forks, pull requests and CI/CD. The types of digital assets maintained via GIT vary from software, deployment scripts, configuration files, documents, website content, metadata records up to actual datasets. Git is most effective with text based formats, which explains the popularity of formats like CSV, YAML, Markdown."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#cicd",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#cicd",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "Continuous Integration & Deployment describes a proces in which changes in software or configuration are automatically tested and deployed to a relevant environment. These processes are commonly facilited by GIT environments. With every commit to the Git repository an action is triggered which runs some tasks."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#github-pages-exersize",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#github-pages-exersize",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "This exersize introduces the CI-CD topic by setting up a basic markdown website in Github Pages, maintained through Git. Markdown is a popular format to store text with annotations on Git.The site will be based on Quarto. Quarto is one of many platforms to generate a website from a markdown repository.\n\nCreate a new repository in your github account, for example ‘My first CMS’. Tick the ’’\nBefore we add any content create a branch ‘gh-pages’ on the repository, this branch will later contain the generated html sources of the website.\nCreate a file _quarto.yml into the new git repository, with this content:\n\nproject:\n  type: website\nwebsite:\n  title: \"hello world\"\n  navbar:\n    left:\n      - href: index.md\n        text: Home\n      - about.md\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nCreate file docs/index.md and docs/about.md. Start each file with a header:\n\n---\ntitle: Hello World\nauthor: Peter pan\ndate: 2023-11-11\n---\nAdd some markdown content to each page (under the header), for example:\n# Welcome\n\nWelcome to *my website*.\n\n- I hope you enjoy it.\n- Visit also my [about](./about.md) page.\n\nNow click on Actions in the github menu. Notice that Github has already set up a workflow to publish our content using jekyll, it should already be available at https://user.github.io/repo. We will not use this approach for now.\nRemove the existing workflow, generated by Github in Actions, Workflows, Remove\nFirst you need to allow the workflow-runner to make changes on the repository. For this, open Settings, Actions, General. Scroll down to Workflow permissions. Tick the Read and write permissions and click Save. If the option is grayed out, you first need to allow this feature in your organization.\nThen, from Actions, select New workflow, then set up a workflow yourself.\nOn the next page we will create a new workflow script, which is stored in the repository at /.github/workflows/main.yml.\n\nname: Docs Deploy\n\non:\n  push:\n    branches: \n      - main\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with: \n          tinytex: true \n          path: docs\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: docs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      \n\nSave the file, via actions you can follow the progress of the workflow at every push to the repository.\nOn the logs notice how a container is initialised, the source code is checked out, the mkdocs dependency is installed, the build is made and pushed to the gh-pages branch.\n\nNotice that the syntax to define workflows is different for every CI-CD platform, however they generally follow a similar pattern. For Github identify in the file above:\n\nIt defines at what events the workflow should trigger (in this case at push events).\na build job is triggered, which indicates a container image (runs-on) to run the job in, then triggers some steps.\nThe final step triggers a facility of quarto to publish its output to a github repository\n\nThe above setup is optimal for co-creating a documentation repository for your community. Users can visit the source code via the edit on github link and suggest improvements via issues of pull requests. Notice that this tutorial is also maintained as markdown in Git."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "For this scenario we need a database in the cloud to host our records (which is reachable by github workflows). For the training we suggest to use a trial account at elephantsql.com.\n\nAt elephantsql, create a new account.\nThen create a new Instance of type Tiny (free).\nClick on the instance and notice the relevant connection string (URL) and password\nConnect your instance of pycsw to this database instance, by updating pycsw.cfg and following the instructions at Catalogue publication\nVerify in elephantsql dashboard if the records are correctly loaded (or install and configure pgadmin).\n\nWe will now publish our records from Github to our database.\n\nCreate a new repository on Github for the records\nMake sure git-scm (or a GUI tool like Git kraken, Gitlab) is intalled on your system (alternatively upload the files through the github web interface)\nClone (download) the repository to a local folder.\n\ngit clone https://github.com/username/records-repo.git\n\nCopy the mcf files, which have been generated in Catalogue publication, to a datasets folder in the cloned repository.\nCommit the files\n\ngit add -A && git commit -m \"Your Message\"\nBefore you can push your changes to Github, you need to set up authentication, generally 2 options are possible: - Using a personal access token - Or using SSH public key\ngit push origin main\nWe’ll now set up CI-CD to publish the records\n\nPlace the pycsw.cfg file in the root of the repositry (including the postgres database connection)\nCreate a new custom workflow file with this content:\n\nname: Records Deploy\n\non: \n  push:\n    paths:\n      - '**'\n\ndefaults:\n  run:\n    working-directory: .\n\njobs:\n  build:\n    name: Build and Deploy Records\n    runs-on: ubuntu-latest\n    steps:\n        - uses: actions/checkout@v3\n        - uses: actions/setup-python@v4\n          with:\n              python-version: 3.9\n        - name: Install dependencies\n          run: |\n            sudo add-apt-repository ppa:ubuntugis/ppa\n            sudo apt-get update\n            sudo apt-get install gdal-bin\n            sudo apt-get install libgdal-dev\n            ogrinfo --version\n            pip install GDAL==3.4.3\n            pip install geodatacrawler pycsw sqlalchemy\n        - name: Crawl metadata\n          run: |\n            export pgdc_webdav_url=http://localhost/collections/metadata:main/items\n            export pgdc_canonical_url=https://github.com/pvgenuchten/data-training/tree/main/datasets/\n            crawl-metadata --dir=./datasets --mode=export --dir-out=/tmp\n        - name: Publish records\n          run: |   \n            pycsw-admin.py delete-records --config=./pycsw.cfg -y\n            pycsw-admin.py load-records --config=./pycsw.cfg  --path=/tmp\n\nVerify that the records are loaded on pycsw (through postgres)\nChange or add some records to GIT, and verify if the changes are published (may take some time)\n\nNormally, one would not add a connection string to a database in a config file posted on Github. Instead Github offers secrets to capture this type of information."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#cross-linking-catalogue-and-git",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#cross-linking-catalogue-and-git",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "While users are browsing the catalogue (or this page), they may find irregularities in the content. They can flag this as an issue in the relevant Git repository. A nice feature is to add a link in the catalogue page which brings them back to the relevant mcf in the git repository. With proper authorisations they can instantly improve the record, or suggest an improvement via an issue or pull request."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#summary",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#summary",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "In this section you learned about using actions in Github (CI/CD). In the next section we are diving into OGC Map Services using mapserver. Notice that you can use these CI/CD mechanisms to deploy or evaluate metadata and data services."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html",
    "title": "Catalogue publication",
    "section": "",
    "text": "Catalogues facilitate data discovery in 3 ways:\n\nUsers can go to the catalogue website and search for data\nApplications such as QGIS and TerriaJS can let users query the catalogue, evaluate the metadata, and directly add the related data to their project\nSearch engines crawl public catalogues and include the records in their search results\n\n\n\n\n\n\n\nNote\n\n\n\nAn important aspect is proper setup of autorisations for general public, partners and co-workers to access metadata as well as the actual data files behind the metadata. A general rule-of-thumb is that metadata can usually be widely shared, but data services with sensitive content should be properly protected. In some cases organisations even remove the data url from the public metadata, to prevent abuse of those urls. If a resource is not available to all, this can be indicated in metadata as ‘access-constraints’.\n\n\n\n\n\nNow that we have a series of datasets with their sidecar metadata file (either in iso19139 or MCF). We can use GeoDataCrawler to extract these files and publish them in a catalogue product.\nNotice that GeoDataCrawler has an advanced inheriting mechanism to find default values for non provided metadata elements. GeoDataCrawler will read index.yml in the current folder and any parent folder to find relevant properties. This enables an option to provide a contact point, license or language property once and use it in all metadata.\n\nFirst we run a method to convert all .mcf files to iso19139:2007, these files are generated on a temporary folder.\n\ncrawl-metadata --mode=export --dir=./data --dir-out=./temp\n\n\n\n\nVarious catalogue frontends exist to facilitate dataset search, such as geonetwork, dataverse, ckan. Selecting a frontend depends on metadata format, target audience, types of data, maintenance aspects, personal preference.\nFor this workshop we are going to use pycsw. It is a catalogue software supporting various standardised query api’s, as well as providing a basic easy-to-adjust html web interface.\nFor this exersize we assume you have docker-desktop installed on your system and running.\npycsw is available as docker image, including an embedded SQLite database. In a production situation you would use a dedicated Postgres or MariaDB database for record storage.\n\nNavigate your shell to the temporary folder containing iso-xml documents. This folder will be mounted into the container, in order to load the records to the pycsw database.\n\ndocker run -p 8000:8000 \\\n   -v $(pwd):/etc/data \\\n   geopython/pycsw\n\nVisit http://localhost:8000\nMuch of the configuration of pycsw (title, contact details, database connection, url) is managed in a config file. Download the file to the current folder, adjust the title and restart docker with:\n\ndocker run -p 8000:8000 \\\n   -d -rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   geopython/pycsw\n\n\n\n\n\n\nNote\n\n\n\nNotice -d starts the docker in the background, so we can interact with the running container. To see which instances are running (in the background) use docker ps. docker logs pycsw shows the logs of a container and docker stop pycsw stops the container. The -rm option removes the container at stop, so we can easily recreate it with additional options at next runs.\n\n\n\nFor administering the instance we use a utility called pycsw-admin.py. Notice on the calls below a reference to a relevant config file.\nFirst clear the existing database:\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\"\n\nNotice at http://localhost:8000/collections/metadata:main/items that all records are removed.\nLoad our records to the database:\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py load-records -p /etc/conf/data -c /etc/pycsw/pycsw.cfg -y -r\"\n\nValidate at http://localhost:8000/collections/metadata:main/items if our records are loaded, else check logs to identify a problem.\n\n\n\n\n\nIn this paragraph you learned how datasets can be published into a catalogue. In the next paragraph, you’ll get introduced to providing mapping api’s on datasets. Or check out the advanced section on how to customise your pycsw frontend."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#extract-metadata-from-a-network-location",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#extract-metadata-from-a-network-location",
    "title": "Catalogue publication",
    "section": "",
    "text": "Now that we have a series of datasets with their sidecar metadata file (either in iso19139 or MCF). We can use GeoDataCrawler to extract these files and publish them in a catalogue product.\nNotice that GeoDataCrawler has an advanced inheriting mechanism to find default values for non provided metadata elements. GeoDataCrawler will read index.yml in the current folder and any parent folder to find relevant properties. This enables an option to provide a contact point, license or language property once and use it in all metadata.\n\nFirst we run a method to convert all .mcf files to iso19139:2007, these files are generated on a temporary folder.\n\ncrawl-metadata --mode=export --dir=./data --dir-out=./temp"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#catalogue-frontend",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#catalogue-frontend",
    "title": "Catalogue publication",
    "section": "",
    "text": "Various catalogue frontends exist to facilitate dataset search, such as geonetwork, dataverse, ckan. Selecting a frontend depends on metadata format, target audience, types of data, maintenance aspects, personal preference.\nFor this workshop we are going to use pycsw. It is a catalogue software supporting various standardised query api’s, as well as providing a basic easy-to-adjust html web interface.\nFor this exersize we assume you have docker-desktop installed on your system and running.\npycsw is available as docker image, including an embedded SQLite database. In a production situation you would use a dedicated Postgres or MariaDB database for record storage.\n\nNavigate your shell to the temporary folder containing iso-xml documents. This folder will be mounted into the container, in order to load the records to the pycsw database.\n\ndocker run -p 8000:8000 \\\n   -v $(pwd):/etc/data \\\n   geopython/pycsw\n\nVisit http://localhost:8000\nMuch of the configuration of pycsw (title, contact details, database connection, url) is managed in a config file. Download the file to the current folder, adjust the title and restart docker with:\n\ndocker run -p 8000:8000 \\\n   -d -rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   geopython/pycsw\n\n\n\n\n\n\nNote\n\n\n\nNotice -d starts the docker in the background, so we can interact with the running container. To see which instances are running (in the background) use docker ps. docker logs pycsw shows the logs of a container and docker stop pycsw stops the container. The -rm option removes the container at stop, so we can easily recreate it with additional options at next runs.\n\n\n\nFor administering the instance we use a utility called pycsw-admin.py. Notice on the calls below a reference to a relevant config file.\nFirst clear the existing database:\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\"\n\nNotice at http://localhost:8000/collections/metadata:main/items that all records are removed.\nLoad our records to the database:\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py load-records -p /etc/conf/data -c /etc/pycsw/pycsw.cfg -y -r\"\n\nValidate at http://localhost:8000/collections/metadata:main/items if our records are loaded, else check logs to identify a problem."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#summary",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#summary",
    "title": "Catalogue publication",
    "section": "",
    "text": "In this paragraph you learned how datasets can be published into a catalogue. In the next paragraph, you’ll get introduced to providing mapping api’s on datasets. Or check out the advanced section on how to customise your pycsw frontend."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-metadata at source.html",
    "href": "docs/developer/tutorial-data-management/1-metadata at source.html",
    "title": "Metadata at the source",
    "section": "",
    "text": "The FAIR principles are designed with the academic community in mind. Researchers sharing information between univeristies. However the FAIR principles can also be applied within an origanisation to improve data management, even on your personal computer. You will notice even you yourself will benefit, if you return for example to an archived project 2 years later to recover some data sources.\nMany organisations organise their documents and datasets on a central network storage or databases. These resources are usually clustered in organisational units, projects and/or periods. Some files and database tables in that central storage contain inherent metadata, such as the name, size, date, author, location etc. This information supports users in understanding the context of the data source. Especcially if that data at some point is migrated from its original context.\nFor those formats which do not have embedded metadata, or in order to capture additional metadata aspects. We endorse the creation of a sidecar metadata file for every resource, a dedicated metadata file sharing the name of the datasource. This approach is for example common in the ESRI community, where a .shp.xml is created alongside any .shp or .tiff file, which captures some metadata elements.\n\n\n\n\n\n\nTip\n\n\n\nLocate on your local computer or network drive a random shapefile. Does the file have a .shp.xml sidecar file? Else find another shape of tiff file (look for *.shp.xml). The xml file may be very limited, but in most cases at least some processing steps and the data model of the shapefile are mentioned.\n\n\nThrough the embedded or sidecar concept, we endorse data scientists to document their data at the source. Since they know best how the data is produced and how it should be used. In this workshop, we are going to use the sidecar convention, to build up a discovery service for datasets within an organisation, with minimal effort for data scientists.\n\n\n\nFor optimal interoperability, it is important to agree within your group on the metadata standard to use in sidecar files. ESRI software for example provides an option to select the output model of the metadata. QGIS has various plugins, such as GeoCat Bridge, to work with various metadata models.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation (or regulation relevant to your role) endorse a metadata model to describe data sources? Are your aware of tooling which can support you in creation of metadata in this model?\n\n\n\n\n\n\nWithin the geopython community a metadata format is used called the metadata control file (MCF). Aim of the format is ease of use, while providing export options to various metadata models. Many metadate models are based on XML, which makes them quite hard to read by humans. MCF is based on YAML, a textbased format using indents to cluster elements. In this workshop we are using the MCF format due to its simplicity and natural fit with the use cases. A minimal sample of MCF is:\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 9c36a048-4d28-453f-9373-94c90e101ebe\n    hierarchylevel: dataset\n    date: 2023-05-10\n\nidentification:\n    title: My favourite dataset\n    abstract: A sample dataset record to highlight the options of MCF\n    ...\nIn the next exercises, we are going to use a combination of MCF and iso19139:2007 to describe datasets (and alternate resources). iso19139:2007 is a standardised metadata format, commonly used in the spatial data community. Notice that MCF can also be combined with alternate metadata models, such as DCAT and STAC.\n\n\n\n\nIn this exersize we’ll extract relevant metadata from a file repository using a crawling tool. Later we will load this metadata into a searchable catalogue. We have prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\n\nNotice a index.yml in the root folder. The tool we use is able to inherit metadata properties from index.yml files through the file hierarchy. Open index.yml and customise the contact details. Later you will notice that these details will be applied to all datasets which themselves do not provide contact details. Consider to add additional index.yml files in other folders to override the values of index.yml at top level.\n\nThe tool we will use is based on python. It has some specific dependencies which are best installed via Conda or mamba. Conda creates a virtual python environment, so any activity will not interfere with the base python environment of your machine.\nIf you don’t have Conda yet, you can install Anaconda or Miniconda and consider to read the getting started. Even more efficient is micromamba\nNow start a commandline or powershell with conda enabled (or add conda to your PATH). First we will navigate to the folder in which we unzipped the sample data repository. Make sure you are not in the data directory but one above.\ncd {path-where-you-unzipped-zipfile}\nWe will create a virtual environment (using Python 3.9) for our project and activate it.\nconda create --name pgdc python=3.9 \nconda activate pgdc\nNotice that you can deactivate this environment with: conda deactivate and you will return to the main Python environment. The tools we will install below, will not be available in the main environment.\nInstall the dependencies for the tool:\nconda install -c conda-forge gdal==3.3.2\nconda install -c conda-forge pysqlite3==0.4.6\nNow install the crawler tool, GeoDataCrawler. The tool is under active development at ISRIC and facilitates many of our data workflows. It is powered by some popular metadata and transformation libraries; OWSLib, pygeometa and GDAL.\npip install geodatacrawler\nVerify the different crawling options by typing:\ncrawl-metadata --help\nThe initial task for the tool is to create for every data file in our repository a sidecar file based on embedded metadata from the resource.\ncrawl-metadata --mode=init --dir=data\nNotice that for each resource a {dataset}.yml file has been created. Open a .yml file in a text editor and review its content.\nThe update mode is meant to be run at intervals, it will update the mcf files if changes have been made on a resource.\ncrawl-metadata --mode=update --dir=data\nIn certain cases the update mode will also import metadata from remote url’s. This happens for example if the dataset-uri is a DOI. The update mode will ten fetch metadata of the DOI and push it into the MCF.\nTo enable remote updates, add the --resolve=true parameter.\ncrawl-metadata --mode=update --dir=data --resolve=true\nFinally we want to export the MCF’s to actual iso19139 metadata to be loaded into a catalogue.\ncrawl-metadata --mode=export --dir=data --dir-out=export --dir-out-mode=flat\nOpen one of the xml files and evaluate if the contact information from step 1 is available.\n\n\n\n\nIn this section, you are introduced to a data management approach which maintains metadata at the location where the datasets are maintained, using a minimal, standards complient approach. You are introduced to the MCF metadata format and to the geodatacrawler tool which is able to create and operate on large numbers of MCF files. In the next section, we will go into more detail on the MCF format and metadata authoring in general."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-metadata at source.html#standardised-metadata-models",
    "href": "docs/developer/tutorial-data-management/1-metadata at source.html#standardised-metadata-models",
    "title": "Metadata at the source",
    "section": "",
    "text": "For optimal interoperability, it is important to agree within your group on the metadata standard to use in sidecar files. ESRI software for example provides an option to select the output model of the metadata. QGIS has various plugins, such as GeoCat Bridge, to work with various metadata models.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation (or regulation relevant to your role) endorse a metadata model to describe data sources? Are your aware of tooling which can support you in creation of metadata in this model?"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-metadata at source.html#the-mcf-format",
    "href": "docs/developer/tutorial-data-management/1-metadata at source.html#the-mcf-format",
    "title": "Metadata at the source",
    "section": "",
    "text": "Within the geopython community a metadata format is used called the metadata control file (MCF). Aim of the format is ease of use, while providing export options to various metadata models. Many metadate models are based on XML, which makes them quite hard to read by humans. MCF is based on YAML, a textbased format using indents to cluster elements. In this workshop we are using the MCF format due to its simplicity and natural fit with the use cases. A minimal sample of MCF is:\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 9c36a048-4d28-453f-9373-94c90e101ebe\n    hierarchylevel: dataset\n    date: 2023-05-10\n\nidentification:\n    title: My favourite dataset\n    abstract: A sample dataset record to highlight the options of MCF\n    ...\nIn the next exercises, we are going to use a combination of MCF and iso19139:2007 to describe datasets (and alternate resources). iso19139:2007 is a standardised metadata format, commonly used in the spatial data community. Notice that MCF can also be combined with alternate metadata models, such as DCAT and STAC."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-metadata at source.html#discovering-an-existing-data-repository",
    "href": "docs/developer/tutorial-data-management/1-metadata at source.html#discovering-an-existing-data-repository",
    "title": "Metadata at the source",
    "section": "",
    "text": "In this exersize we’ll extract relevant metadata from a file repository using a crawling tool. Later we will load this metadata into a searchable catalogue. We have prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\n\nNotice a index.yml in the root folder. The tool we use is able to inherit metadata properties from index.yml files through the file hierarchy. Open index.yml and customise the contact details. Later you will notice that these details will be applied to all datasets which themselves do not provide contact details. Consider to add additional index.yml files in other folders to override the values of index.yml at top level.\n\nThe tool we will use is based on python. It has some specific dependencies which are best installed via Conda or mamba. Conda creates a virtual python environment, so any activity will not interfere with the base python environment of your machine.\nIf you don’t have Conda yet, you can install Anaconda or Miniconda and consider to read the getting started. Even more efficient is micromamba\nNow start a commandline or powershell with conda enabled (or add conda to your PATH). First we will navigate to the folder in which we unzipped the sample data repository. Make sure you are not in the data directory but one above.\ncd {path-where-you-unzipped-zipfile}\nWe will create a virtual environment (using Python 3.9) for our project and activate it.\nconda create --name pgdc python=3.9 \nconda activate pgdc\nNotice that you can deactivate this environment with: conda deactivate and you will return to the main Python environment. The tools we will install below, will not be available in the main environment.\nInstall the dependencies for the tool:\nconda install -c conda-forge gdal==3.3.2\nconda install -c conda-forge pysqlite3==0.4.6\nNow install the crawler tool, GeoDataCrawler. The tool is under active development at ISRIC and facilitates many of our data workflows. It is powered by some popular metadata and transformation libraries; OWSLib, pygeometa and GDAL.\npip install geodatacrawler\nVerify the different crawling options by typing:\ncrawl-metadata --help\nThe initial task for the tool is to create for every data file in our repository a sidecar file based on embedded metadata from the resource.\ncrawl-metadata --mode=init --dir=data\nNotice that for each resource a {dataset}.yml file has been created. Open a .yml file in a text editor and review its content.\nThe update mode is meant to be run at intervals, it will update the mcf files if changes have been made on a resource.\ncrawl-metadata --mode=update --dir=data\nIn certain cases the update mode will also import metadata from remote url’s. This happens for example if the dataset-uri is a DOI. The update mode will ten fetch metadata of the DOI and push it into the MCF.\nTo enable remote updates, add the --resolve=true parameter.\ncrawl-metadata --mode=update --dir=data --resolve=true\nFinally we want to export the MCF’s to actual iso19139 metadata to be loaded into a catalogue.\ncrawl-metadata --mode=export --dir=data --dir-out=export --dir-out-mode=flat\nOpen one of the xml files and evaluate if the contact information from step 1 is available."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-metadata at source.html#summary",
    "href": "docs/developer/tutorial-data-management/1-metadata at source.html#summary",
    "title": "Metadata at the source",
    "section": "",
    "text": "In this section, you are introduced to a data management approach which maintains metadata at the location where the datasets are maintained, using a minimal, standards complient approach. You are introduced to the MCF metadata format and to the geodatacrawler tool which is able to create and operate on large numbers of MCF files. In the next section, we will go into more detail on the MCF format and metadata authoring in general."
  },
  {
    "objectID": "docs/developer/terria.html",
    "href": "docs/developer/terria.html",
    "title": "terria js",
    "section": "",
    "text": "TerriaJS is a web based vizualisation tool. This document describes how terriajs has been customised and deployed to fit the needs of LSC Hubs.\nTerriaMap is a ready-made web environment for the the terriaJS library.\nA docker container definition for TerriaMap is available at github.\nOn the container a number of files need to be overridden with tailored changes (to update titles, logo’s colors).\n\nwwwroot/about.html contains the text of the about page\nwwwroot/config.json contains the config of the main application\ninit/simple.json and other map files in this folder can be used as predefined maps\n\nYou can either override these files as part of the container build process or add the files st container deployment."
  },
  {
    "objectID": "docs/developer/pycsw.html",
    "href": "docs/developer/pycsw.html",
    "title": "pycsw",
    "section": "",
    "text": "pycsw is an effective catalogue implementation in python, supporting a range of record exchange standards for optimal interoperability."
  },
  {
    "objectID": "docs/admin/index.html",
    "href": "docs/admin/index.html",
    "title": "Administration",
    "section": "",
    "text": "The current hub is published via Github. Github is a commercial service provider offering GIT services plus a number of add-ons to facilitate co-creation. Alternative GIT providers could be considered, such as gitlab, bitbucket. Notice that it is also possible to install gitlab on premise.\nAs an LSC hub administrator it is relevant to familiarise yourself with Github. Notice that some tools exist to facilitate you on the use of Git, such as Github desktop, smartgit and git kraken. Some prefer the Github plugin of Visual Studio.\n\n\n\nA number of content pages is made available to guide the user within the hub. These pages are maintained in github as markdown. Notice the edit link in the footer of each page. The pages are rendered to html using the quarto tool. Editing of quarto documents is facilitated by a quarto plugin in visual studio.\nThe quarto tool is configured as a ci-cd action in github. You can follow the progress of an action in the actions tab. In some cases an action may fail and human interaction is needed to fix the problem and/or restart the action.\n\n\n\nDiscussions are managed within github. A widget is added to every resource page, so users can provide feedback or ask questions about the resource. These questions are stored as github discussion. Discussions can be answered from the resource page (a github login is required) or from github discussions. Users should be invited to the github project to be able to administer github discussions.\n\n\n\nThe TerriaJS framework enables users to share maps and map stories with stakeholders (share button top-right). Notice that you can also use this functionality to link from a description of a resource to a map, which displays some data in a certain context.\n[A nice map](https://maps.lsc-hubs.org/#start=%7B%22version%22%...)\nAnother option is to store the map definition (urldecoded) on a folder of TerriaJS. The map is then available by its filename. You can now add a link to this map for example in the related maps section of TerriaJS (config.json).\n\n\n\nThe catalogue can be linked to various information sources, without the data being stored on the LSC hub. Therefore, if you know existing information sources that are missing and should be added. Proving metadata is essential for findability and to avoid ambiguity.\nSource of catalogue records is maintained at github. Records are organised in Global, Continental, and country. Within country they are organised by portal/initiative.\nMetadata records are stored in the mcf format, a subset of iso19115 in a conveniant yaml encoding. But you can also add them as iso19115 xml format.\nA number of mechanisms is available to load records into the catalogue.\n\nImport records from external sources, such as data portals (zenodo, dataverse, CSW, STAC, OSF)\nMcf records can be created using mdme\nA Excel template is available, on which resources can be described. A single resource per record.\nAn ODK form is available on which users can describe resources\n\n\n\n\nODK"
  },
  {
    "objectID": "docs/admin/index.html#github",
    "href": "docs/admin/index.html#github",
    "title": "Administration",
    "section": "",
    "text": "The current hub is published via Github. Github is a commercial service provider offering GIT services plus a number of add-ons to facilitate co-creation. Alternative GIT providers could be considered, such as gitlab, bitbucket. Notice that it is also possible to install gitlab on premise.\nAs an LSC hub administrator it is relevant to familiarise yourself with Github. Notice that some tools exist to facilitate you on the use of Git, such as Github desktop, smartgit and git kraken. Some prefer the Github plugin of Visual Studio."
  },
  {
    "objectID": "docs/admin/index.html#lsc-hub",
    "href": "docs/admin/index.html#lsc-hub",
    "title": "Administration",
    "section": "",
    "text": "A number of content pages is made available to guide the user within the hub. These pages are maintained in github as markdown. Notice the edit link in the footer of each page. The pages are rendered to html using the quarto tool. Editing of quarto documents is facilitated by a quarto plugin in visual studio.\nThe quarto tool is configured as a ci-cd action in github. You can follow the progress of an action in the actions tab. In some cases an action may fail and human interaction is needed to fix the problem and/or restart the action."
  },
  {
    "objectID": "docs/admin/index.html#discussions",
    "href": "docs/admin/index.html#discussions",
    "title": "Administration",
    "section": "",
    "text": "Discussions are managed within github. A widget is added to every resource page, so users can provide feedback or ask questions about the resource. These questions are stored as github discussion. Discussions can be answered from the resource page (a github login is required) or from github discussions. Users should be invited to the github project to be able to administer github discussions."
  },
  {
    "objectID": "docs/admin/index.html#maps",
    "href": "docs/admin/index.html#maps",
    "title": "Administration",
    "section": "",
    "text": "The TerriaJS framework enables users to share maps and map stories with stakeholders (share button top-right). Notice that you can also use this functionality to link from a description of a resource to a map, which displays some data in a certain context.\n[A nice map](https://maps.lsc-hubs.org/#start=%7B%22version%22%...)\nAnother option is to store the map definition (urldecoded) on a folder of TerriaJS. The map is then available by its filename. You can now add a link to this map for example in the related maps section of TerriaJS (config.json)."
  },
  {
    "objectID": "docs/admin/index.html#catalogue",
    "href": "docs/admin/index.html#catalogue",
    "title": "Administration",
    "section": "",
    "text": "The catalogue can be linked to various information sources, without the data being stored on the LSC hub. Therefore, if you know existing information sources that are missing and should be added. Proving metadata is essential for findability and to avoid ambiguity.\nSource of catalogue records is maintained at github. Records are organised in Global, Continental, and country. Within country they are organised by portal/initiative.\nMetadata records are stored in the mcf format, a subset of iso19115 in a conveniant yaml encoding. But you can also add them as iso19115 xml format.\nA number of mechanisms is available to load records into the catalogue.\n\nImport records from external sources, such as data portals (zenodo, dataverse, CSW, STAC, OSF)\nMcf records can be created using mdme\nA Excel template is available, on which resources can be described. A single resource per record.\nAn ODK form is available on which users can describe resources\n\n\n\n\nODK"
  },
  {
    "objectID": "cases/services.html",
    "href": "cases/services.html",
    "title": "Services",
    "section": "",
    "text": "Services"
  },
  {
    "objectID": "cases/isfm.html",
    "href": "cases/isfm.html",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Current fertilizer and soil fertility recommendations are not linked to local context, conditions and climate risks. Too general information may lead to declining soil fertility and soil health, low and uncertain productivity, fertilizer wastage, loss of organic matter, and environmental risks. Integrated Soil Fertility Management (ISFM) has the potential to improve effectivity and efficiency of agronomic practices including both fertilizer recommendations and organic matter management and thereby boost crop production and farm income. Also, ISFM has the potential to deliver multiple co-benefits including climate adaptation (through improved water holding capacity and soil cover) as well as climate mitigation (through carbon sequestration).​\n\n\n\nImproved agronomic management advisories including crop type and soil fertility management to farmers via agricultural extension staff or directly.​\n\n\n\nLand users and their intermediaries can use the system to obtain information on the soil fertility and alternatives to improve soil fertility. It is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and models (WOFOST, DSSAT etc) accessible in the LSC-hub. These will include risk assessment of weather and other external factors.​\n\n\n\n\nField/farm,\nSub-national (county/district/woreda)\nNational levels​\n\n\n\n\n\nnational: Extension staff, agricultural planner, agro-input provider\ndistrict: District extension officer, District Agric & Livestock officers and planners\nlocal: Local extension officer, local agro-input provider, lead farmer.​\n​\n\n\n\n\n\nHub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​\n\n\n\n\n\nFarmers.​\n\n\n\n\nThe intermediary user accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor – extension agent/ lead farmer/ agricultural input provider - converts this information into an agronomic and fertilizer recommendation and provides advise to end users, in particular farmers.​\n​"
  },
  {
    "objectID": "cases/isfm.html#setting",
    "href": "cases/isfm.html#setting",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Current fertilizer and soil fertility recommendations are not linked to local context, conditions and climate risks. Too general information may lead to declining soil fertility and soil health, low and uncertain productivity, fertilizer wastage, loss of organic matter, and environmental risks. Integrated Soil Fertility Management (ISFM) has the potential to improve effectivity and efficiency of agronomic practices including both fertilizer recommendations and organic matter management and thereby boost crop production and farm income. Also, ISFM has the potential to deliver multiple co-benefits including climate adaptation (through improved water holding capacity and soil cover) as well as climate mitigation (through carbon sequestration).​"
  },
  {
    "objectID": "cases/isfm.html#goal",
    "href": "cases/isfm.html#goal",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Improved agronomic management advisories including crop type and soil fertility management to farmers via agricultural extension staff or directly.​"
  },
  {
    "objectID": "cases/isfm.html#scope",
    "href": "cases/isfm.html#scope",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Land users and their intermediaries can use the system to obtain information on the soil fertility and alternatives to improve soil fertility. It is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and models (WOFOST, DSSAT etc) accessible in the LSC-hub. These will include risk assessment of weather and other external factors.​"
  },
  {
    "objectID": "cases/isfm.html#scale",
    "href": "cases/isfm.html#scale",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Field/farm,\nSub-national (county/district/woreda)\nNational levels​"
  },
  {
    "objectID": "cases/isfm.html#primary-actors",
    "href": "cases/isfm.html#primary-actors",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "national: Extension staff, agricultural planner, agro-input provider\ndistrict: District extension officer, District Agric & Livestock officers and planners\nlocal: Local extension officer, local agro-input provider, lead farmer.​\n​"
  },
  {
    "objectID": "cases/isfm.html#secondary-actors",
    "href": "cases/isfm.html#secondary-actors",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Hub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​"
  },
  {
    "objectID": "cases/isfm.html#beneficiary",
    "href": "cases/isfm.html#beneficiary",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "Farmers.​"
  },
  {
    "objectID": "cases/isfm.html#scenario",
    "href": "cases/isfm.html#scenario",
    "title": "Kenya Integrated Soil Fertility Management description​",
    "section": "",
    "text": "The intermediary user accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor – extension agent/ lead farmer/ agricultural input provider - converts this information into an agronomic and fertilizer recommendation and provides advise to end users, in particular farmers.​\n​"
  },
  {
    "objectID": "cases/models.html",
    "href": "cases/models.html",
    "title": "models",
    "section": "",
    "text": "Models"
  },
  {
    "objectID": "cases/swc.html",
    "href": "cases/swc.html",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Soil erosion is a major threat to sustainability and productivity with knock-on effects on the climate crisis and food security. It decreases soil fertility negatively affecting crop yields on fields, while eroded sediments from upstream clog downstream, dams, rivers and canals causing sedimentation and flooding. This is particularly true for hilly areas in Ethiopia, Kenya and Rwanda with high erosion risk. Soil erosion can be prevented and arrested through soil and water conservation (SWC) or sustainable land management (SLM) practices. Current land use and land management practices including crop cultivation as well as grazing do not sufficiently consider local potential, limitations and climate and soil erosion risks. While general information on improved land management may exist, context-based information on land resources, and climate and information on feasible SLM alternatives is not widely available or applied by land managers. ​ ​\n\n\n\nStakeholders informed of status of soil erosion risks and motivated to apply and promote more sustainable land use and land management practices through improved policies, plans and agricultural extension services, including contributions to Land Degradation Neutrality (LDN) monitoring and reporting.​ ​\n\n\n\nCatchment managers, authorities and farmer organisations can use the system to obtain information on the suitability of various sustainable land uses and land management practices for agricultural production and land restoration. These is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and GIS based land suitability and soil erosion models, accessible in the LSC-hub.​\n\n\n\nLSC-IS hub, land suitability applications, Soil erosion and AEZ models and data used in national LDN monitoring and plans for County development, catchment management and farm management.​\n\n\n\n\nNational\nsub-national, landscape or catchment\nlocal level​ ​ ## Primary actors\nNational: national and regional agriculture and forestry/natural resource management authorities, planners and project managers, UNCCD focal points.​\nLocal: catchment management, agricultural extension staff, farmer’s and other community-based organisations and authorities.​\n\n\n\n\nHub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​\n\n\n\nFarmers and communities.​\n\n\n\nThe intermediary actor accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor converts this information into land use and land management recommendations and advise to ultimate end users, in particular farmers.​"
  },
  {
    "objectID": "cases/swc.html#setting",
    "href": "cases/swc.html#setting",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Soil erosion is a major threat to sustainability and productivity with knock-on effects on the climate crisis and food security. It decreases soil fertility negatively affecting crop yields on fields, while eroded sediments from upstream clog downstream, dams, rivers and canals causing sedimentation and flooding. This is particularly true for hilly areas in Ethiopia, Kenya and Rwanda with high erosion risk. Soil erosion can be prevented and arrested through soil and water conservation (SWC) or sustainable land management (SLM) practices. Current land use and land management practices including crop cultivation as well as grazing do not sufficiently consider local potential, limitations and climate and soil erosion risks. While general information on improved land management may exist, context-based information on land resources, and climate and information on feasible SLM alternatives is not widely available or applied by land managers. ​ ​"
  },
  {
    "objectID": "cases/swc.html#goal",
    "href": "cases/swc.html#goal",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Stakeholders informed of status of soil erosion risks and motivated to apply and promote more sustainable land use and land management practices through improved policies, plans and agricultural extension services, including contributions to Land Degradation Neutrality (LDN) monitoring and reporting.​ ​"
  },
  {
    "objectID": "cases/swc.html#scope",
    "href": "cases/swc.html#scope",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Catchment managers, authorities and farmer organisations can use the system to obtain information on the suitability of various sustainable land uses and land management practices for agricultural production and land restoration. These is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and GIS based land suitability and soil erosion models, accessible in the LSC-hub.​"
  },
  {
    "objectID": "cases/swc.html#system",
    "href": "cases/swc.html#system",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "LSC-IS hub, land suitability applications, Soil erosion and AEZ models and data used in national LDN monitoring and plans for County development, catchment management and farm management.​"
  },
  {
    "objectID": "cases/swc.html#scale",
    "href": "cases/swc.html#scale",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "National\nsub-national, landscape or catchment\nlocal level​ ​ ## Primary actors\nNational: national and regional agriculture and forestry/natural resource management authorities, planners and project managers, UNCCD focal points.​\nLocal: catchment management, agricultural extension staff, farmer’s and other community-based organisations and authorities.​"
  },
  {
    "objectID": "cases/swc.html#secondary-actors",
    "href": "cases/swc.html#secondary-actors",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Hub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​"
  },
  {
    "objectID": "cases/swc.html#beneficiary",
    "href": "cases/swc.html#beneficiary",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Farmers and communities.​"
  },
  {
    "objectID": "cases/swc.html#scenario",
    "href": "cases/swc.html#scenario",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "The intermediary actor accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor converts this information into land use and land management recommendations and advise to ultimate end users, in particular farmers.​"
  },
  {
    "objectID": "docs/developer/index.html",
    "href": "docs/developer/index.html",
    "title": "Developer documentation",
    "section": "",
    "text": "Content management with Quarto\npycsw catalogue\nterriajs"
  },
  {
    "objectID": "docs/developer/index.html#contents",
    "href": "docs/developer/index.html#contents",
    "title": "Developer documentation",
    "section": "",
    "text": "Content management with Quarto\npycsw catalogue\nterriajs"
  },
  {
    "objectID": "docs/developer/index.html#alternative-relevant-content",
    "href": "docs/developer/index.html#alternative-relevant-content",
    "title": "Developer documentation",
    "section": "Alternative relevant content",
    "text": "Alternative relevant content\n\nEJP Soil Date Assimilaton Cookbook\nTutorial data management"
  },
  {
    "objectID": "docs/developer/quarto.html",
    "href": "docs/developer/quarto.html",
    "title": "quarto",
    "section": "",
    "text": "Quarto is a content management framework. Content is typically maintained as markdown files on a git repository. With every push to the git repository a new set of html pages is build and published to an online environment, for example github.io.\nread more about quarto at Quarto"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html",
    "title": "FAIR principles",
    "section": "",
    "text": "The FAIR principles are a good practice on data sharing in academic communities and beyond. A good starting point for our workshop on data management.\n\nOutside academia there have been similar initiatives to improve data management, which may be more relevant for dedicated communities or scenarios:\n\nW3C Data on the web best practices\nOpen government data principles\nEuropean INSPIRE Directive\n\n\nIn this paragraph we present a number of exercises around the FAIR principles. And we’ll see how the principles work out in the soil domain specifically. FAIR Data is:\n\nFindable\nAccessible\nInteroperable\nReusable\n\n\n\n\nMetadata and data should be easy to find for both humans and computers.\n\n\n\nA local identifier is usually combined with a namespace, to create a globally unique identifier (URI).\nDo not use product names, project names, group names in uri’s, it is difficult to maintain persistence.\nFrameworks such as DOI and W3ID offer a persistent identification layer for online resources.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the following 2 types of datasets, review the uniqueness and persistence of their identifier as well as of their metadata.\n\nLocate some datasets on your local machine or organisation network.\nLocate some datasets on the web. If you don’t know where to start your search, consider to read the catalogue section first.\n\n\n\n\nDataset\nIdentifier\nYour review\n\n\n\n\nSample\na432-bcd-4ab55\nNo namespace\n\n\n\n\n\n\n\n\nRich metadata includes aspects such as, title, abstract, keywords, who is the author/owner of the resource, when was the resource created, are there any usage constraints, how does the resource relate to other resources.\n\n\n\n\n\n\nTip\n\n\n\nTo evaluate if a tiff-dataset containing texture-clay is relevant to answer your question on soil water availability, which aspects would you expect a metadata description of that dataset to include?\n\n\n\n\n\nIn order to find metadata efficiently, metadata records should be listed in a intuitive search interface\n\n\n\n\n\n\nTip\n\n\n\nNavigate to the following data portals and search for a dataset on for example soil texture in your area. Note down which aspects you would like to see improved to locate a dataset, or to know when to stop searching, because you assume you have located the best match in the catalogue.\n\nGeoportal.org\nGoogle dataset search\nData.isric.org\nDigitalearth.africa\nFAO catalogue\n\n\n\n\n\n\n\n\nOnce a user finds the data, it should be clear how they can be accessed.\n\n\nVarious communities adopted a range of standards for metadata exchange:\n\n\n\n\n\nCommunity\nStandard\nProtocol\n\n\n\n\nOpen data/Sematic web\nDCAT\nSPARQL\n\n\nScience\nDatacite\nOAI-PMH\n\n\nGeospatial\niso19115\nCSW/OGC API - Records\n\n\nEarth observation\nSTAC\nSTAC\n\n\nSearch engines\nSchema.org\njson-ld/microdata\n\n\nEcology\nEML\nKNB/GBIF\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA metadata model often is a combination of a schema and a format. Compare the following metadata records, identify which model is used in the record, what differences and communalities do you notice?\n\nAfrican open soil data\nWosis latest\nRainfall Chirps\nSoil excavation Assen\nKeileem Drenthe\n\n\n\n\n\n\nMost common in data science is to provide a packaged version of a dataset and deploy it on a repository like Zenodo or Dataverse where it can be downloaded. However in the spatial and earth observation domain we tend to work with large files and the use of data api’s which allow to request subsets of the data are very common. The Open Geospatial Consortium has defined a number of standards for these API’s, so the API’s itself are interoperable. The table below shows some of the common API’s. In the first column the older API’s, developed in the 90’s, in the second column their updated representative, recently adopted or still in development.\n\n\n\nService\nOGC API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualisation of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorthings\nRetrieve subsets of sensor observations\n\n\n\nFrom the Earth Observation domain, an alternative mechanism is increasingly getting adopted. Complete files are stored on a public file repository, but by creating an index on the file and enabling range requests, users are able to fetch subsets from the file directly.\nThis mechanism is enabled by new formats such as Cloud Optimised GeoTiff, GeoZarr, and GeoParquet.\n\n\n\n\nFAIR endorses open access, however in some cases it is not possible to share some data to the global audiance (privacy, economic, or safety concerns). It is still relevant to publish the data, so those authorised can access it. This requires a proper level of autorisation and authentication being set up.\n\n\n\nMetadata models usually have a status field, which enables you to indicate that a resource has been archived. The metadata would still be available, so users are aware it once existed.\n\n\n\n\n\nData typically are integrated with other data, as well as interoperate with applications or workflows for analysis, storage, and processing.\n\n\nThe soil community has a long history of interoperability efforts for soil profile data. Such as:\n\nGlobalsoilmap\ne-Soter\niso28258:2013\n\n\n\nThe e-Soter model has been developed in the e-Soter Research project, based on principles of previous SOil TERrain (SOTER) initiatives. e-Soter is a relational database model, usually implemented as a Microsoft Access database. Some examples of eSoter implementations:\n\nMalawi\nKenya\nSouthern Africa\nSenegal and Gambia\n\n\n\n\nIn 2012 various experts in the soil domain grouped around the development of the first formally standardised domain model on soil data, published as ISO28258. ISO28258 adopted the Observations & Measurements conventions of OGC. Each observation on a site, profile, horizon or soil sample is considered as an observation. For each observation on a specimen, the measured property and the procedure are captured.\n\n\n\nObservations and measurements overview\n\n\nVarious initiatives adopted ISO28258, and serialised and specialised the model for their community:\n\nINSPIRE Soil (Europe) and ANZSoilML (Australia) are implementations of iso28258 serialised as GML.\nISO28258-relational is an implementation of ISO28258 modelled as a relational database.\nGlosis Web Ontology is an iso28258 implementation, using common ontologies from the web, such as semantic sensor network.\n\nSome examples of iso28258 implementations:\n\nSoil Berlin\nSoil chemistry Flanders\nSoil Poland\n\n\n\n\n\n\n\nTip\n\n\n\nDownload a Soil GML file and try to open it in QGIS. QGIS usually is able to display the profile locations. Alternatively you can use the GML Appschema format in OGR to generate first a SQLite database of the file, before opening it in QGIS.\n\n\n\n\n\n\nA number of common vocabularies are relevant to the soil domain.\nThe World Reference Base for soil resources provides a framework of soil vocabularies. These lists are partially published in Agrovoc and partially in the Glosis web ontology.\n\n\n\n\n\n\nTip\n\n\n\nExamine the concept Durisols in agrovoc.\n\nNotice that the agrovoc page on Durisols looks much nicer, still it is important to use the persistent identifier when linking to the concept, why?\nNotice that Agrovoc contains many translations for each concept and linkage to wider and narrower terms. These are some of the benefits of linking to a keyword from a common thesaurus.\n\n\n\n\n\n\nThe context of a dataset gets more clear if you link it to datasets which were used as a source, documents in which it is described, tools with which it has been produced or which tool can be used to view/process it, policies for which it has been created, etc. Consider that users also may traverse the link, to find datasets relevant to a certain policy or tool.\n\n\n\n\n\nReuse of data is the main goal of FAIR, facilitated by documentation of the data, for different audiences.\n\n\nUsers are very interested to know if and how they can use the data. This process is facilitated by adoption of a commonly available license, such as odbl or cc-by, so users (and machines) can identify the applicable license without reading a full document.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation provide guidance on which license to use on various data sources? Is it clear when, and when not to use an open license? Are you aware of any data sources which currently do not yet have an assigned data license?\n\n\n\n\n\nProvenance is the process of creation and curation of a data source. Which data sources or procedures were used to create the data source. Which processing steps have been applied to the data. What is the lifecycle of the dataset (when will it be archived).\nThis information is very relevant to potential users of the data, because they can understand if the data has been produced according to their expectations.\nIn academia provenance is usually described in scientific articles, but note that you can also capture it (with much more detail) in a metadata record of a data source. Some tools (for example ArcGIS and SPSS) create a processing log automatically.\n\n\n\n\n\nIn this section you learned about the FAIR principles and how this applies to the soil data community. In the next sections we will introduce a data management strategy we use on some of our projects. We expect some of the presented tools may be worthwile to have a closer look at, to see if it can support you in your daily tasks."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#findable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#findable",
    "title": "FAIR principles",
    "section": "",
    "text": "Metadata and data should be easy to find for both humans and computers.\n\n\n\nA local identifier is usually combined with a namespace, to create a globally unique identifier (URI).\nDo not use product names, project names, group names in uri’s, it is difficult to maintain persistence.\nFrameworks such as DOI and W3ID offer a persistent identification layer for online resources.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the following 2 types of datasets, review the uniqueness and persistence of their identifier as well as of their metadata.\n\nLocate some datasets on your local machine or organisation network.\nLocate some datasets on the web. If you don’t know where to start your search, consider to read the catalogue section first.\n\n\n\n\nDataset\nIdentifier\nYour review\n\n\n\n\nSample\na432-bcd-4ab55\nNo namespace\n\n\n\n\n\n\n\n\nRich metadata includes aspects such as, title, abstract, keywords, who is the author/owner of the resource, when was the resource created, are there any usage constraints, how does the resource relate to other resources.\n\n\n\n\n\n\nTip\n\n\n\nTo evaluate if a tiff-dataset containing texture-clay is relevant to answer your question on soil water availability, which aspects would you expect a metadata description of that dataset to include?\n\n\n\n\n\nIn order to find metadata efficiently, metadata records should be listed in a intuitive search interface\n\n\n\n\n\n\nTip\n\n\n\nNavigate to the following data portals and search for a dataset on for example soil texture in your area. Note down which aspects you would like to see improved to locate a dataset, or to know when to stop searching, because you assume you have located the best match in the catalogue.\n\nGeoportal.org\nGoogle dataset search\nData.isric.org\nDigitalearth.africa\nFAO catalogue"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#accessible",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#accessible",
    "title": "FAIR principles",
    "section": "",
    "text": "Once a user finds the data, it should be clear how they can be accessed.\n\n\nVarious communities adopted a range of standards for metadata exchange:\n\n\n\n\n\nCommunity\nStandard\nProtocol\n\n\n\n\nOpen data/Sematic web\nDCAT\nSPARQL\n\n\nScience\nDatacite\nOAI-PMH\n\n\nGeospatial\niso19115\nCSW/OGC API - Records\n\n\nEarth observation\nSTAC\nSTAC\n\n\nSearch engines\nSchema.org\njson-ld/microdata\n\n\nEcology\nEML\nKNB/GBIF\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA metadata model often is a combination of a schema and a format. Compare the following metadata records, identify which model is used in the record, what differences and communalities do you notice?\n\nAfrican open soil data\nWosis latest\nRainfall Chirps\nSoil excavation Assen\nKeileem Drenthe\n\n\n\n\n\n\nMost common in data science is to provide a packaged version of a dataset and deploy it on a repository like Zenodo or Dataverse where it can be downloaded. However in the spatial and earth observation domain we tend to work with large files and the use of data api’s which allow to request subsets of the data are very common. The Open Geospatial Consortium has defined a number of standards for these API’s, so the API’s itself are interoperable. The table below shows some of the common API’s. In the first column the older API’s, developed in the 90’s, in the second column their updated representative, recently adopted or still in development.\n\n\n\nService\nOGC API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualisation of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorthings\nRetrieve subsets of sensor observations\n\n\n\nFrom the Earth Observation domain, an alternative mechanism is increasingly getting adopted. Complete files are stored on a public file repository, but by creating an index on the file and enabling range requests, users are able to fetch subsets from the file directly.\nThis mechanism is enabled by new formats such as Cloud Optimised GeoTiff, GeoZarr, and GeoParquet.\n\n\n\n\nFAIR endorses open access, however in some cases it is not possible to share some data to the global audiance (privacy, economic, or safety concerns). It is still relevant to publish the data, so those authorised can access it. This requires a proper level of autorisation and authentication being set up.\n\n\n\nMetadata models usually have a status field, which enables you to indicate that a resource has been archived. The metadata would still be available, so users are aware it once existed."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#interoperable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#interoperable",
    "title": "FAIR principles",
    "section": "",
    "text": "Data typically are integrated with other data, as well as interoperate with applications or workflows for analysis, storage, and processing.\n\n\nThe soil community has a long history of interoperability efforts for soil profile data. Such as:\n\nGlobalsoilmap\ne-Soter\niso28258:2013\n\n\n\nThe e-Soter model has been developed in the e-Soter Research project, based on principles of previous SOil TERrain (SOTER) initiatives. e-Soter is a relational database model, usually implemented as a Microsoft Access database. Some examples of eSoter implementations:\n\nMalawi\nKenya\nSouthern Africa\nSenegal and Gambia\n\n\n\n\nIn 2012 various experts in the soil domain grouped around the development of the first formally standardised domain model on soil data, published as ISO28258. ISO28258 adopted the Observations & Measurements conventions of OGC. Each observation on a site, profile, horizon or soil sample is considered as an observation. For each observation on a specimen, the measured property and the procedure are captured.\n\n\n\nObservations and measurements overview\n\n\nVarious initiatives adopted ISO28258, and serialised and specialised the model for their community:\n\nINSPIRE Soil (Europe) and ANZSoilML (Australia) are implementations of iso28258 serialised as GML.\nISO28258-relational is an implementation of ISO28258 modelled as a relational database.\nGlosis Web Ontology is an iso28258 implementation, using common ontologies from the web, such as semantic sensor network.\n\nSome examples of iso28258 implementations:\n\nSoil Berlin\nSoil chemistry Flanders\nSoil Poland\n\n\n\n\n\n\n\nTip\n\n\n\nDownload a Soil GML file and try to open it in QGIS. QGIS usually is able to display the profile locations. Alternatively you can use the GML Appschema format in OGR to generate first a SQLite database of the file, before opening it in QGIS.\n\n\n\n\n\n\nA number of common vocabularies are relevant to the soil domain.\nThe World Reference Base for soil resources provides a framework of soil vocabularies. These lists are partially published in Agrovoc and partially in the Glosis web ontology.\n\n\n\n\n\n\nTip\n\n\n\nExamine the concept Durisols in agrovoc.\n\nNotice that the agrovoc page on Durisols looks much nicer, still it is important to use the persistent identifier when linking to the concept, why?\nNotice that Agrovoc contains many translations for each concept and linkage to wider and narrower terms. These are some of the benefits of linking to a keyword from a common thesaurus.\n\n\n\n\n\n\nThe context of a dataset gets more clear if you link it to datasets which were used as a source, documents in which it is described, tools with which it has been produced or which tool can be used to view/process it, policies for which it has been created, etc. Consider that users also may traverse the link, to find datasets relevant to a certain policy or tool."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#reusable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#reusable",
    "title": "FAIR principles",
    "section": "",
    "text": "Reuse of data is the main goal of FAIR, facilitated by documentation of the data, for different audiences.\n\n\nUsers are very interested to know if and how they can use the data. This process is facilitated by adoption of a commonly available license, such as odbl or cc-by, so users (and machines) can identify the applicable license without reading a full document.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation provide guidance on which license to use on various data sources? Is it clear when, and when not to use an open license? Are you aware of any data sources which currently do not yet have an assigned data license?\n\n\n\n\n\nProvenance is the process of creation and curation of a data source. Which data sources or procedures were used to create the data source. Which processing steps have been applied to the data. What is the lifecycle of the dataset (when will it be archived).\nThis information is very relevant to potential users of the data, because they can understand if the data has been produced according to their expectations.\nIn academia provenance is usually described in scientific articles, but note that you can also capture it (with much more detail) in a metadata record of a data source. Some tools (for example ArcGIS and SPSS) create a processing log automatically."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#summary",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#summary",
    "title": "FAIR principles",
    "section": "",
    "text": "In this section you learned about the FAIR principles and how this applies to the soil data community. In the next sections we will introduce a data management strategy we use on some of our projects. We expect some of the presented tools may be worthwile to have a closer look at, to see if it can support you in your daily tasks."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-describing-resources.html",
    "href": "docs/developer/tutorial-data-management/2-describing-resources.html",
    "title": "Describing resources",
    "section": "",
    "text": "When describing a resource, consider which user groups are expected to read the information. This analyses will likely impact the style of writing in the metadata. The UK Geospatial Commission has published some practical recommendations on this topic.\n\nWhen tagging the dataset with keywords, preferably use keywords from controlled vocabularies like Agrovoc, Wikipedia, etc. The benefit of controlled vocabularies is that the term is not ambigue and it can be made available in multiple languages.\n\n\n\n\nMCF documents can best be written in a text editor like Visual Studio Code. Consider to install the YAML plugin for instant YAML validation.\nAnother option to create and update mcf files is via MDME. MDME is a webbased software package providing a dynamic metadata edit form. An operational package is available at osgeo.github.io. Notice that if you install the package locally, you can customize the metadata schema and initial template to your organisational needs.\n\n\n\n\n\n\nTip\n\n\n\nOpen mdme and populate the form, now save the MCF file and place it in your sample data repository. Notice that MDME also offers capabilities to export directly as iso19139, it uses a server side process based on the same components as we use in this workshop.\n\n\n\n\n\n\npygeometa is the library build around the mcf format. pygeometa has options to import from and export to mcf for various metadata schemas. pygeometa also has a validate method to validate if an mcf document is properly structured.\npygeometa has already been installed, while installing geodatacrawler, else you can install it with pip install pygeometa.\n\n\n\n\n\n\nTip\n\n\n\nTry the validator on one of your mcf files.\npygeometa validate path/to/file.yml\n\n\nNotice that the validator is quite strict, it expects a number of metadata properties to be present. We’ll further not use the validator in this workshop.\n\n\n\n\nIn this paragraph you are introduced to the MDME MCF editor and pygeometa library. In the next section we are looking at catalogue publication."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-describing-resources.html#mcf-editing",
    "href": "docs/developer/tutorial-data-management/2-describing-resources.html#mcf-editing",
    "title": "Describing resources",
    "section": "",
    "text": "MCF documents can best be written in a text editor like Visual Studio Code. Consider to install the YAML plugin for instant YAML validation.\nAnother option to create and update mcf files is via MDME. MDME is a webbased software package providing a dynamic metadata edit form. An operational package is available at osgeo.github.io. Notice that if you install the package locally, you can customize the metadata schema and initial template to your organisational needs.\n\n\n\n\n\n\nTip\n\n\n\nOpen mdme and populate the form, now save the MCF file and place it in your sample data repository. Notice that MDME also offers capabilities to export directly as iso19139, it uses a server side process based on the same components as we use in this workshop."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-describing-resources.html#mcf-validation",
    "href": "docs/developer/tutorial-data-management/2-describing-resources.html#mcf-validation",
    "title": "Describing resources",
    "section": "",
    "text": "pygeometa is the library build around the mcf format. pygeometa has options to import from and export to mcf for various metadata schemas. pygeometa also has a validate method to validate if an mcf document is properly structured.\npygeometa has already been installed, while installing geodatacrawler, else you can install it with pip install pygeometa.\n\n\n\n\n\n\nTip\n\n\n\nTry the validator on one of your mcf files.\npygeometa validate path/to/file.yml\n\n\nNotice that the validator is quite strict, it expects a number of metadata properties to be present. We’ll further not use the validator in this workshop."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-describing-resources.html#summary",
    "href": "docs/developer/tutorial-data-management/2-describing-resources.html#summary",
    "title": "Describing resources",
    "section": "",
    "text": "In this paragraph you are introduced to the MDME MCF editor and pygeometa library. In the next section we are looking at catalogue publication."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html",
    "title": "Bulk import",
    "section": "",
    "text": "This paragraph describes approaches to import metadata from existing repositories. Including an option to import metadata from records of a spreadsheet.\n\n\n\nMany metadata initiatives tend to start from a spreadsheet. Each of the columns representa a metadata property and the rows are the individual records describing a resource. Spreadsheets have proven to be an effective medium to populate a catalogue with records. To facilitate this use case the GeoDataCrawler software provides an import spreadsheet method. The spreadsheet is parsed and a MCF document is generated for every row.\nSince every metadata initiative tends to have dedicated columns. A templating approach is used to convert from row to MCF. A default template is available, matching a default spreadsheet layout. If your spreadsheet layout is different, you need to adjust the template accordingly.\n\nFor this exercise we’ll use the LSC-hubs spreadsheet in combination with the LSC-hubs template. Notice that the template share the filename, but with extension .j2. Download both files to a new folder, called csv, in your working directory.\nFrom your shell environment run this command:\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\"\n\nIf there are errors, check the paths and consider to open the CSV in Google Sheets and export it again or open it in a text editor to look for special cases. A known issue with this approach is that the crawler tool can not manage newline characters in text fields.\nOpen one of the generated MCF files to evaluate its content.\nA common spreadsheet tool is Microsoft Excel. If you open and export a spreadsheet from Excel, the CSV will use the ‘;’ character as column separator. Use the –sep=‘;’ parameter to indicate GeoDataCrawler to use this separator.\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\" --sep=';'\n\n\n\n\nMany resources are already described elsewhere which may be of interest to add to our catalogue. For this use case some options exist to import remote metadata. In many cases you want to import a selected subset of a remote catalogue. In this scenario we prepare a csv with the identifiers of the records to be imported. These identifiers are used to create a very minimal MCF file. GeoDataCrawler subsequently extends the local record with remote content. Currently supported are metadata from CSW, WMS and DOI.\n\nFor this exercise download a spreadsheet with a subset of the isric.org data repository and the relevant template to a folder isric.\nImport the CSV\n\ncrawl-metadata --mode=import-csv --dir=\"./isric\" --sep=';'\n\nThe synchronisation with remote content is implemented in GeoDataCrawler as part of the update metadata method. The update process will evaluate the dataseturi-property. If the uri refers to remote content in a supported format, the MCF will be updated against that content (remote takes preference).\n\ncrawl-metadata --mode=update --dir=\"./isric\"\n\nNotice the changes before and after running the script. If needed, you can remove all the MCF files and run import-csv again to restore the originals."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-spreadsheet",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-spreadsheet",
    "title": "Bulk import",
    "section": "",
    "text": "Many metadata initiatives tend to start from a spreadsheet. Each of the columns representa a metadata property and the rows are the individual records describing a resource. Spreadsheets have proven to be an effective medium to populate a catalogue with records. To facilitate this use case the GeoDataCrawler software provides an import spreadsheet method. The spreadsheet is parsed and a MCF document is generated for every row.\nSince every metadata initiative tends to have dedicated columns. A templating approach is used to convert from row to MCF. A default template is available, matching a default spreadsheet layout. If your spreadsheet layout is different, you need to adjust the template accordingly.\n\nFor this exercise we’ll use the LSC-hubs spreadsheet in combination with the LSC-hubs template. Notice that the template share the filename, but with extension .j2. Download both files to a new folder, called csv, in your working directory.\nFrom your shell environment run this command:\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\"\n\nIf there are errors, check the paths and consider to open the CSV in Google Sheets and export it again or open it in a text editor to look for special cases. A known issue with this approach is that the crawler tool can not manage newline characters in text fields.\nOpen one of the generated MCF files to evaluate its content.\nA common spreadsheet tool is Microsoft Excel. If you open and export a spreadsheet from Excel, the CSV will use the ‘;’ character as column separator. Use the –sep=‘;’ parameter to indicate GeoDataCrawler to use this separator.\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\" --sep=';'"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-an-online-location",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-an-online-location",
    "title": "Bulk import",
    "section": "",
    "text": "Many resources are already described elsewhere which may be of interest to add to our catalogue. For this use case some options exist to import remote metadata. In many cases you want to import a selected subset of a remote catalogue. In this scenario we prepare a csv with the identifiers of the records to be imported. These identifiers are used to create a very minimal MCF file. GeoDataCrawler subsequently extends the local record with remote content. Currently supported are metadata from CSW, WMS and DOI.\n\nFor this exercise download a spreadsheet with a subset of the isric.org data repository and the relevant template to a folder isric.\nImport the CSV\n\ncrawl-metadata --mode=import-csv --dir=\"./isric\" --sep=';'\n\nThe synchronisation with remote content is implemented in GeoDataCrawler as part of the update metadata method. The update process will evaluate the dataseturi-property. If the uri refers to remote content in a supported format, the MCF will be updated against that content (remote takes preference).\n\ncrawl-metadata --mode=update --dir=\"./isric\"\n\nNotice the changes before and after running the script. If needed, you can remove all the MCF files and run import-csv again to restore the originals."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html",
    "title": "Data publication",
    "section": "",
    "text": "In order to share a dataset with colleagues, partners or the wider public. The file should be published in a shared environment. Various technologies are available to share a file on a network. To select a relevant location mainly depends on which type of users are going to access the dataset.\nThe following options exist:\n\nA shared folder on a central server on the local intranet. Notice that this location is usually not available by remote partners. Make sure a backup is made of the network folder, in case of incidents.\nA cloud service such as Google Drive, Mirosoft Sharepoint, Dropbox, Amazon Webservices. Such a service can also be setup locally. A minimal solution would be to set up a Webdav service.\nA data repository such as Zenodo, Dataverse, Open Science Foundation. With this option metadata of the resource is automatically collected and made searchable. Some catalogue software, such as CKAN, GeoNode, GeoNetwork offers the capability to publish data files as part of the metadata registration.\n\n\n\n\nSelect the location carefully, prevent to regularly move a file to a new location. Because with every move, users need to be notified of the new location and documentation needs to be updated. When moving is relevant, consider to set up a forward-rule on the server which redirects users to the new location.\nAn interesting aspect of persistent identification is the choice of a domain and path name. A domain should represent enough credibility/authority (is this a trusted resource), and should be persistent for a longer period. A project name, for example, is not a good choice for a domain to publish a resource.\nA mechanism exists which facilitates file moves, without breaking their identification. Identifier providers such as DOI and W3ID enable to create a identifier for any resource. In documentation use the provided identifier, in case the location of the resource changes, you can update the link behind the identifier. Some organisations install a identification service themselves, so they have full ownership on the domain and the contents of the service. An example of such a service is the identification service of the German Federal Government.\n\n\n\n\nFor optimal discoverability, it is important to combine data publication with metadata. Either via embedded metadata in the file, else with a separate metadata file. In case of a shared folder or cloud service, embed or place the metadata along the data files, so people browsing through the system can easily find it.\nThe embedded or sidecar metadata should be duplicated into catalogue software, to make it searchable by the targeted audience. This proces is further described at catalogue publication.\n\n\n\n\nVarious technologies exist to share data on a network. When selecting a mechanism, evaluate if you can facilitate identifier persistence and share metadata along with the files. In the next section we’ll setup conveniance API’s on data to facilitate reuse of the data."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#persistent-identification",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#persistent-identification",
    "title": "Data publication",
    "section": "",
    "text": "Select the location carefully, prevent to regularly move a file to a new location. Because with every move, users need to be notified of the new location and documentation needs to be updated. When moving is relevant, consider to set up a forward-rule on the server which redirects users to the new location.\nAn interesting aspect of persistent identification is the choice of a domain and path name. A domain should represent enough credibility/authority (is this a trusted resource), and should be persistent for a longer period. A project name, for example, is not a good choice for a domain to publish a resource.\nA mechanism exists which facilitates file moves, without breaking their identification. Identifier providers such as DOI and W3ID enable to create a identifier for any resource. In documentation use the provided identifier, in case the location of the resource changes, you can update the link behind the identifier. Some organisations install a identification service themselves, so they have full ownership on the domain and the contents of the service. An example of such a service is the identification service of the German Federal Government."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#include-metadata",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#include-metadata",
    "title": "Data publication",
    "section": "",
    "text": "For optimal discoverability, it is important to combine data publication with metadata. Either via embedded metadata in the file, else with a separate metadata file. In case of a shared folder or cloud service, embed or place the metadata along the data files, so people browsing through the system can easily find it.\nThe embedded or sidecar metadata should be duplicated into catalogue software, to make it searchable by the targeted audience. This proces is further described at catalogue publication."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#conclusion",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#conclusion",
    "title": "Data publication",
    "section": "",
    "text": "Various technologies exist to share data on a network. When selecting a mechanism, evaluate if you can facilitate identifier persistence and share metadata along with the files. In the next section we’ll setup conveniance API’s on data to facilitate reuse of the data."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html",
    "title": "Quality of Service",
    "section": "",
    "text": "Quality of service monitoring practices support data providers to understand strengths and weaknesses of a system. Aspects which are monitored are:\n\nAvailability (% of the time that the service has been available)\nPerformance and capacity\nUsage (how much are the services used)\n\nQuality of service monitoring is a standard activity in IT. Therefore consult your IT department or hosting company if they have tools available to assess these aspects. Confirm with them on how to extend and/or share with you these measurements for the requested parameters. Combine these reports into monthly or quarterly reports to facilitate policy development. Below exersizes\n\n\n\nTo assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is usually sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom. A special mention for the Python based GeoHealthCheck package, which includes the capability on WMS/WFS services to drill down to the data level starting from the GetCapabilities operation.\nThis exersize assumes docker desktop to be installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click register in the login page).\n\nStart by setting up a local GeoHealthCheck container:\n\ndocker run --name ghc -p 80:80 geopython/geohealthcheck\n\nVisit http://localhost\nLogin as user: admin, password: admin\nClick ADD + on the top bar right, select WMS\nAdd a WMS url, for example https://maps.isric.org/mapserv?map=/map/wrb.map\nOn the next screen add WMS Drilldown (so all layers are validated)\nClick Save and test\nWhen finished, click Details to see the test result\n\nThis test is automatically repeated at intervals (as long the service is running). You can return to the test page to evaluate a diagram of availability over time.\n\n\n\n\nTo know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like Kibana and evaluate the number of requests exceeding the limits.\n[!NOTE]\nA common challenge to service performance is the provision of a WMS service on a big dataset. When requesting that dataset on a continental or national level, the server runs into problems drawing all the data at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators may be a solution also.\njmeter is a utility which can run a series of performance and capacity tests on a webservice. Jmeter is a java program, which can run on most platforms.\n\nDownload the latest version from the apache website.\nUnzip the archive and run jmeter.bat from bin directory.\nFollow the build web test plan tutorial.\nCustomise the web test plan for your mapserver service\n\n[!NOTE]\nDo not perform a load test against a production url, it wil severely impact the performance of that service.\n\n\n\n\nTo capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk, Matomo or AW stats. For spatial data, it is interesting to define rules to extract the requested layer name from a WMS request.\nAWStats is a basic utility to report on service usage.\n\nNavigate to an empty folder, place a sample log file in the folder, rename the file to access.log.\nStart a container\n\ndocker run -d --restart always --publish 3000:80 --name awstats --volume $(pwd):/var/local/log:ro pabra/awstats\n\nParse the logs:\n\ndocker exec awstats awstats_updateall.pl now\n\nView the dashboard at http://localhost:3000, navigate to May 2015 to see the parsed logs from the sample.\n\n\n\n\nAs part of usage monitoring it is also of interest to understand how users find your data via search engines. The popular search engines offer tooling to report on how the crawlers navigate through your data and how users find your services. You need to verify ownership of a domain either via a DNS property or by uploading an identification string to the website.\nYou can also use Google rich result test to extract structured data from a website. Or go to the dataset search engine to understand if and how the search engine finds your data.\n\n\n\n\n\nIn this section you learned about various mechanisms to evaluate, report about and improve service quality. This concludes the main part of the workshop. In the last section we collected some advanced options in case you want to learn more details about data management."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#availability-monitoring-exercise",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#availability-monitoring-exercise",
    "title": "Quality of Service",
    "section": "",
    "text": "To assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is usually sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom. A special mention for the Python based GeoHealthCheck package, which includes the capability on WMS/WFS services to drill down to the data level starting from the GetCapabilities operation.\nThis exersize assumes docker desktop to be installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click register in the login page).\n\nStart by setting up a local GeoHealthCheck container:\n\ndocker run --name ghc -p 80:80 geopython/geohealthcheck\n\nVisit http://localhost\nLogin as user: admin, password: admin\nClick ADD + on the top bar right, select WMS\nAdd a WMS url, for example https://maps.isric.org/mapserv?map=/map/wrb.map\nOn the next screen add WMS Drilldown (so all layers are validated)\nClick Save and test\nWhen finished, click Details to see the test result\n\nThis test is automatically repeated at intervals (as long the service is running). You can return to the test page to evaluate a diagram of availability over time."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#performance-capacity-testing",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#performance-capacity-testing",
    "title": "Quality of Service",
    "section": "",
    "text": "To know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like Kibana and evaluate the number of requests exceeding the limits.\n[!NOTE]\nA common challenge to service performance is the provision of a WMS service on a big dataset. When requesting that dataset on a continental or national level, the server runs into problems drawing all the data at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators may be a solution also.\njmeter is a utility which can run a series of performance and capacity tests on a webservice. Jmeter is a java program, which can run on most platforms.\n\nDownload the latest version from the apache website.\nUnzip the archive and run jmeter.bat from bin directory.\nFollow the build web test plan tutorial.\nCustomise the web test plan for your mapserver service\n\n[!NOTE]\nDo not perform a load test against a production url, it wil severely impact the performance of that service."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#usage-monitoring",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#usage-monitoring",
    "title": "Quality of Service",
    "section": "",
    "text": "To capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk, Matomo or AW stats. For spatial data, it is interesting to define rules to extract the requested layer name from a WMS request.\nAWStats is a basic utility to report on service usage.\n\nNavigate to an empty folder, place a sample log file in the folder, rename the file to access.log.\nStart a container\n\ndocker run -d --restart always --publish 3000:80 --name awstats --volume $(pwd):/var/local/log:ro pabra/awstats\n\nParse the logs:\n\ndocker exec awstats awstats_updateall.pl now\n\nView the dashboard at http://localhost:3000, navigate to May 2015 to see the parsed logs from the sample.\n\n\n\n\nAs part of usage monitoring it is also of interest to understand how users find your data via search engines. The popular search engines offer tooling to report on how the crawlers navigate through your data and how users find your services. You need to verify ownership of a domain either via a DNS property or by uploading an identification string to the website.\nYou can also use Google rich result test to extract structured data from a website. Or go to the dataset search engine to understand if and how the search engine finds your data."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#summary",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#summary",
    "title": "Quality of Service",
    "section": "",
    "text": "In this section you learned about various mechanisms to evaluate, report about and improve service quality. This concludes the main part of the workshop. In the last section we collected some advanced options in case you want to learn more details about data management."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/index.html",
    "href": "docs/developer/tutorial-data-management/index.html",
    "title": "Workshop on effective spatial data flows",
    "section": "",
    "text": "Introduction\nThis document presents a workshop on effective (spatial) data flows. This workshop is most applicable to organisations which maintain (spatial) datasets as files on a network storage, are familiar with python development, and use GIT as an environment for software development, documentation and/or contiuous integration & deployment. However aspects are also relevant in alternate scenario’s.\nThe workshop focusses on evaluating existing mechanisms to discover and assess data resources in an organisation. And extend these mechanisms to increase findability and usability of these resources within and outside the organisation.\nYou can also access the slides of the workshop."
  },
  {
    "objectID": "docs/user/index.html",
    "href": "docs/user/index.html",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "This document is a guideline for usage of the Land Soil Crop hub.\n\n\nDecision support tools in agronomy heavily depend on acurate environmental and crop data. Many data are available, but often disperse and hard to locate. Land Soil Crop hubs aim to faciliate findability and accessibility of relevant data. Hubs facilitate data and knowledge to be stored, described, processed and visualized.\nThe hubs describe a number of different resources relevant to the Land Soil Cop community.\n\n\nDatasets are either a source for predictions, as well as can be the result of a prediction.\n\n\n\nServices are offered by organizations to facilitate access to data and knowlegde. Services can vary from web services (API’s), SMS services, brochures, up to on site visits.\n\n\n\nPredictions on distribution of environmental parameters in time and space, as well as yield predictions based on these distribution, are calculated using statistical, rule based, or machine learning models. Commonly available models are described as resources in the hub.\n\n\n\nIn the hub various software components can be described which either enable you or your stakeholders to visualise and analyse relevant data and knowledge sources\n\n\n\nApproaches descrive commonly known mechanisms to improve agronomy to overcome challenges such as erosion, limited fertility, salinisation, and climate change.\n\n\n\nThe hub describes relevant policies to the Land Soil Crop community. Policy drives the collection of data, but data can also support policy development.\n\n\n\n\nYou can search for data and knowledge in the hub in various ways.\n\nSearch by keyword\nOn a search result, further limit the results by filters in the sidebar\n\n\n\n\nHub catalogue\n\n\nWhen you click on submit, the available records will appear.\n\n\n\nHub catalogue overview\n\n\nThe catalogue allows the filtering of keywords. The 3 main keywords are: 1. category ( such as soil, crop, etc.), 2. spatial scope (such as Global, National, district, etc. ), 3. the type (such as dataset, software, etc).\nSecond, any other keyword linked to the resource can be used to search in the catalogue, for example, land use or crop yield. This depends on which keywords are given to the resources.\n\n\n\n\n\n\nTip\n\n\n\nTry the keywords. Type in the search bar various keywords, such as soil or Land use or click on the keywords on the side.\n\n\n\n\n\nHub catalogue search\n\n\nFor each record, a number of metadata properties are provided, such as abstract, used datasets, keywords, usage constraints, and contact information.\nSome records link directly to the map viewer component. Under the image, it will say: Open record in the LSC map and you will be directed to the map viewer.\n\n\n\n\n\n\nTip\n\n\n\nExplore the records. Click, after searching on keywords, on one of the appeared records and explore the provided information. Click on the links in the records\n\n\n\n\n\nHub catalogue record\n\n\n\n\n\nSpatial data can be viewed and compared in a web-based map viewer. The map viewer can be accessed on the homepage of the LSC hub, under DATA and then click on Go the map viewer. The map viewer can also directly be accessed at https://maps.lsc-hubs.org/#lsc-rwanda\n\n\n\nHub map vizualisation\n\n\nThe map viewer can used for the visualisation of existing maps listed in the LSC catalogue, your data and web data. In this guideline, we will take you through the available functionalities of the map viewer.\n\n\n\n\n\nmapviewer top bar\n\n\n\nGet more information about the map viewer. The introduction, disclaimer and data attributes are described.\nthe related maps show other available maps, such as the LSC Ethiopia, LSC Keny and LSC Rwanda. If you click on one of these maps, it will zoom to those locations. \nMap settings allows you to select a different base map, such as natural earth maps or aerial maps. This depends on which base map you prefer to work with. \nHelp gives useful tips on how to use the map viewer. It provides a tour through the map viewer and a step-by-step guide. This is an interactive guideline and shows the main functionalities. \nA Story is a function that allows you to create and share interactive stories directly from your map. It contains a video with an explanation of how to create them. \nShare/Print generates a link to your created map, which you can share with colleagues. Anything you have added to the map viewer will be shown in the shareable link. You can also use this button to download your created map as an image.\n\n\n\n\n\nThe vertical toolbar allows you to zoom on the map or location, compare maps, measure distance and provide feedback. Each of the buttons will be explained below.\n\n\n\nmapviewer rightbar\n\n\n\nZoom in and out, and back to a full world zoom\nZoom to your current location\nCompare two map data side-by-side. In the next section on explore map data, we go into more detail on this function. \nMeasure the distance on the map between two locations. \nProvide feedback on the map viewer. Feedback is essential to improve the map viewer and to ensure that the map viewer fits the user’s needs. If you have any feedback on the map viewer, you can give it by this button. \n\n\n\n\n\n\n\nTip\n\n\n\nExplore the basic settings of the mapviewer. Change the map settings, take the tour at the help button, download your current map, measure distances and go to your location.\n\n\n\n\n\nThe sidebar is the main location for adding maps to the map viewer and visualising your data or any other web data.\n\nSearch for locations allows you to search for a specific location and go to your area of interest. \nExplore map data shows a listing of datasets that can be added to the map via a catalogue search or directly from available maps. If the panel is empty, select an alternative map from related maps. \nUnder the available maps, you can click on a property map to which you would like to add the viewer. It shows, for example, for the property pH, 4 maps: the 5% prediction value, the 95% prediction value, the median of predictions and the pH map. The values are given for pH*10 for better visualisation. Under the data preview, the metadata of the map is given. You can add the map to the map viewer by clicking on Add to the map. \nYou can add as many maps to the Mapviewer as you want. For example, you add another map of Organic Carbon. \nIf you now click on the compare button as described in the previous section, and put one layer to the left and the other to the right, you can compare the layers side-by-side. \nAbout data brings you back to the Explore map Data, and shows you the metadata describing the map. The description gives in addition which datasets are used to generate the maps.\nUpload provides the option to open a dataset from the local computer. Note that this data is not uploaded to a server, so this data is not shared with anyone else. You can also add web data from this panel to the map viewer. \nFor local files, you first need to select a file type. The file should have a spatial component and/or coordinates to add it to the map viewer. In step 2, you browse your file on your local computer. \nFor web data, you first need to select the file or web service type. In step 2, you will add the URL to add the web data. For example, you can add the ESA land cover map as a WMS layer. The URL is: https://worldcover2020.esa.int/geoserver/gwc/service/wms?SERVICE=WMS&VERSION=1.1.1 \nYou can compare these maps with other added maps, through the compare button. \nAs soon as layers are loaded on the map, you can set the order of the layers, view a legend of the layer, zoom to its extent, set its opacity and view the metadata of the data.\n\n\n\n\n\n\n\nTip\n\n\n\nThe steps of this exercise are written down under the help button. By clicking on Take the tour, it will guide you through the steps.\n\n\n\nSearch for a location to quickly find an area of interest\nUse ‘Explore map data’ to view the catalogue of available data sets and add at least two to the map\nInteract with the data layer, including opacity and toggling on and off on the left in your workbench, compare the maps by using the compare button\nClick on the data on the map to view more detailed data, including the raw data\nChange your base map using options in ‘Map Settings’ to help make some data sets more visible\nZoom and change your view, including tilting the view angle using the controls on the right-hand side of the screen\n\n\n\n\n\nYou are very much invited to contribute to the development of the hub. The contents of the hub is maintained via a co-creation platform called github.com. You can either directly contribute via the github platform, but a feed back mechanism is also provided on each of the hub resources.\nProviding your feedback is crucial for several reasons:\n\nFirstly, incorporating diverse perspectives from stakeholders involved in working with and benefiting from land, soil, and crop information ensures that the LSC hub caters to the actual needs and demands of its users (Data providers/Users). This feedback allows for the fine-tuning of the hub’s functionalities, making it more user-friendly and effective in serving the specific requirements of different user groups.\nSecondly, gathering feedback facilitates continuous improvement. It provides an opportunity to identify potential shortcomings or areas needing enhancement within the LSC hubs. Insights from stakeholders enable the developers and administrators of the hub to address any challenges faced by users, thereby refining the system to better align with the expectations and requirements of its intended beneficiaries.\nMoreover, involving stakeholders in providing feedback fosters a sense of ownership and collaboration. When users feel heard and their inputs valued, it encourages their active participation and engagement with the LSC hub. This collaborative approach promotes a sense of ownership among stakeholders, leading to increased utilization and sustained support for the system in the long term.\nUltimately, the feedback obtained from diverse stakeholders during the Rwanda WP4 workshop plays a pivotal role in ensuring that the LSC hub evolves as a valuable, user-centric platform, effectively supporting decision-making processes related to land, soil, and crop information in Rwanda’s agricultural landscape.\n\nEvery page or resource on the hub provides an option to provide feedback and/or ask a question related to the content. In these sections, you can provide feedback about the page and what you would like to be adjusted.\n\n\n\nfeedback page\n\n\nContributions to the hub require a Github login. A GIThub account is easily made by pressing on sign in with GIThub, then click on New to GIThub? Create an account.\nYou only need to decide on a username, and password and enter your email address.\n\n\n\nfeedback github\n\n\nOnce logged in, you can now comment below the pages. If you have an account, You can provide feedback by contributing to hub discussions at the github repository. To get started, you can create a new discussion.\n\n\n\nfeedback discussion\n\n\nExercise 5: Create a GIThub account and start a discussion at the GIThub repository."
  },
  {
    "objectID": "docs/user/index.html#what-is-a-land-soil-crop-hub",
    "href": "docs/user/index.html#what-is-a-land-soil-crop-hub",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "Decision support tools in agronomy heavily depend on acurate environmental and crop data. Many data are available, but often disperse and hard to locate. Land Soil Crop hubs aim to faciliate findability and accessibility of relevant data. Hubs facilitate data and knowledge to be stored, described, processed and visualized.\nThe hubs describe a number of different resources relevant to the Land Soil Cop community.\n\n\nDatasets are either a source for predictions, as well as can be the result of a prediction.\n\n\n\nServices are offered by organizations to facilitate access to data and knowlegde. Services can vary from web services (API’s), SMS services, brochures, up to on site visits.\n\n\n\nPredictions on distribution of environmental parameters in time and space, as well as yield predictions based on these distribution, are calculated using statistical, rule based, or machine learning models. Commonly available models are described as resources in the hub.\n\n\n\nIn the hub various software components can be described which either enable you or your stakeholders to visualise and analyse relevant data and knowledge sources\n\n\n\nApproaches descrive commonly known mechanisms to improve agronomy to overcome challenges such as erosion, limited fertility, salinisation, and climate change.\n\n\n\nThe hub describes relevant policies to the Land Soil Crop community. Policy drives the collection of data, but data can also support policy development."
  },
  {
    "objectID": "docs/user/index.html#find-data",
    "href": "docs/user/index.html#find-data",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "You can search for data and knowledge in the hub in various ways.\n\nSearch by keyword\nOn a search result, further limit the results by filters in the sidebar\n\n\n\n\nHub catalogue\n\n\nWhen you click on submit, the available records will appear.\n\n\n\nHub catalogue overview\n\n\nThe catalogue allows the filtering of keywords. The 3 main keywords are: 1. category ( such as soil, crop, etc.), 2. spatial scope (such as Global, National, district, etc. ), 3. the type (such as dataset, software, etc).\nSecond, any other keyword linked to the resource can be used to search in the catalogue, for example, land use or crop yield. This depends on which keywords are given to the resources.\n\n\n\n\n\n\nTip\n\n\n\nTry the keywords. Type in the search bar various keywords, such as soil or Land use or click on the keywords on the side.\n\n\n\n\n\nHub catalogue search\n\n\nFor each record, a number of metadata properties are provided, such as abstract, used datasets, keywords, usage constraints, and contact information.\nSome records link directly to the map viewer component. Under the image, it will say: Open record in the LSC map and you will be directed to the map viewer.\n\n\n\n\n\n\nTip\n\n\n\nExplore the records. Click, after searching on keywords, on one of the appeared records and explore the provided information. Click on the links in the records\n\n\n\n\n\nHub catalogue record"
  },
  {
    "objectID": "docs/user/index.html#map-viewer",
    "href": "docs/user/index.html#map-viewer",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "Spatial data can be viewed and compared in a web-based map viewer. The map viewer can be accessed on the homepage of the LSC hub, under DATA and then click on Go the map viewer. The map viewer can also directly be accessed at https://maps.lsc-hubs.org/#lsc-rwanda\n\n\n\nHub map vizualisation\n\n\nThe map viewer can used for the visualisation of existing maps listed in the LSC catalogue, your data and web data. In this guideline, we will take you through the available functionalities of the map viewer.\n\n\n\n\n\nmapviewer top bar\n\n\n\nGet more information about the map viewer. The introduction, disclaimer and data attributes are described.\nthe related maps show other available maps, such as the LSC Ethiopia, LSC Keny and LSC Rwanda. If you click on one of these maps, it will zoom to those locations. \nMap settings allows you to select a different base map, such as natural earth maps or aerial maps. This depends on which base map you prefer to work with. \nHelp gives useful tips on how to use the map viewer. It provides a tour through the map viewer and a step-by-step guide. This is an interactive guideline and shows the main functionalities. \nA Story is a function that allows you to create and share interactive stories directly from your map. It contains a video with an explanation of how to create them. \nShare/Print generates a link to your created map, which you can share with colleagues. Anything you have added to the map viewer will be shown in the shareable link. You can also use this button to download your created map as an image.\n\n\n\n\n\nThe vertical toolbar allows you to zoom on the map or location, compare maps, measure distance and provide feedback. Each of the buttons will be explained below.\n\n\n\nmapviewer rightbar\n\n\n\nZoom in and out, and back to a full world zoom\nZoom to your current location\nCompare two map data side-by-side. In the next section on explore map data, we go into more detail on this function. \nMeasure the distance on the map between two locations. \nProvide feedback on the map viewer. Feedback is essential to improve the map viewer and to ensure that the map viewer fits the user’s needs. If you have any feedback on the map viewer, you can give it by this button. \n\n\n\n\n\n\n\nTip\n\n\n\nExplore the basic settings of the mapviewer. Change the map settings, take the tour at the help button, download your current map, measure distances and go to your location.\n\n\n\n\n\nThe sidebar is the main location for adding maps to the map viewer and visualising your data or any other web data.\n\nSearch for locations allows you to search for a specific location and go to your area of interest. \nExplore map data shows a listing of datasets that can be added to the map via a catalogue search or directly from available maps. If the panel is empty, select an alternative map from related maps. \nUnder the available maps, you can click on a property map to which you would like to add the viewer. It shows, for example, for the property pH, 4 maps: the 5% prediction value, the 95% prediction value, the median of predictions and the pH map. The values are given for pH*10 for better visualisation. Under the data preview, the metadata of the map is given. You can add the map to the map viewer by clicking on Add to the map. \nYou can add as many maps to the Mapviewer as you want. For example, you add another map of Organic Carbon. \nIf you now click on the compare button as described in the previous section, and put one layer to the left and the other to the right, you can compare the layers side-by-side. \nAbout data brings you back to the Explore map Data, and shows you the metadata describing the map. The description gives in addition which datasets are used to generate the maps.\nUpload provides the option to open a dataset from the local computer. Note that this data is not uploaded to a server, so this data is not shared with anyone else. You can also add web data from this panel to the map viewer. \nFor local files, you first need to select a file type. The file should have a spatial component and/or coordinates to add it to the map viewer. In step 2, you browse your file on your local computer. \nFor web data, you first need to select the file or web service type. In step 2, you will add the URL to add the web data. For example, you can add the ESA land cover map as a WMS layer. The URL is: https://worldcover2020.esa.int/geoserver/gwc/service/wms?SERVICE=WMS&VERSION=1.1.1 \nYou can compare these maps with other added maps, through the compare button. \nAs soon as layers are loaded on the map, you can set the order of the layers, view a legend of the layer, zoom to its extent, set its opacity and view the metadata of the data.\n\n\n\n\n\n\n\nTip\n\n\n\nThe steps of this exercise are written down under the help button. By clicking on Take the tour, it will guide you through the steps.\n\n\n\nSearch for a location to quickly find an area of interest\nUse ‘Explore map data’ to view the catalogue of available data sets and add at least two to the map\nInteract with the data layer, including opacity and toggling on and off on the left in your workbench, compare the maps by using the compare button\nClick on the data on the map to view more detailed data, including the raw data\nChange your base map using options in ‘Map Settings’ to help make some data sets more visible\nZoom and change your view, including tilting the view angle using the controls on the right-hand side of the screen"
  },
  {
    "objectID": "docs/user/index.html#hub-community",
    "href": "docs/user/index.html#hub-community",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "You are very much invited to contribute to the development of the hub. The contents of the hub is maintained via a co-creation platform called github.com. You can either directly contribute via the github platform, but a feed back mechanism is also provided on each of the hub resources.\nProviding your feedback is crucial for several reasons:\n\nFirstly, incorporating diverse perspectives from stakeholders involved in working with and benefiting from land, soil, and crop information ensures that the LSC hub caters to the actual needs and demands of its users (Data providers/Users). This feedback allows for the fine-tuning of the hub’s functionalities, making it more user-friendly and effective in serving the specific requirements of different user groups.\nSecondly, gathering feedback facilitates continuous improvement. It provides an opportunity to identify potential shortcomings or areas needing enhancement within the LSC hubs. Insights from stakeholders enable the developers and administrators of the hub to address any challenges faced by users, thereby refining the system to better align with the expectations and requirements of its intended beneficiaries.\nMoreover, involving stakeholders in providing feedback fosters a sense of ownership and collaboration. When users feel heard and their inputs valued, it encourages their active participation and engagement with the LSC hub. This collaborative approach promotes a sense of ownership among stakeholders, leading to increased utilization and sustained support for the system in the long term.\nUltimately, the feedback obtained from diverse stakeholders during the Rwanda WP4 workshop plays a pivotal role in ensuring that the LSC hub evolves as a valuable, user-centric platform, effectively supporting decision-making processes related to land, soil, and crop information in Rwanda’s agricultural landscape.\n\nEvery page or resource on the hub provides an option to provide feedback and/or ask a question related to the content. In these sections, you can provide feedback about the page and what you would like to be adjusted.\n\n\n\nfeedback page\n\n\nContributions to the hub require a Github login. A GIThub account is easily made by pressing on sign in with GIThub, then click on New to GIThub? Create an account.\nYou only need to decide on a username, and password and enter your email address.\n\n\n\nfeedback github\n\n\nOnce logged in, you can now comment below the pages. If you have an account, You can provide feedback by contributing to hub discussions at the github repository. To get started, you can create a new discussion.\n\n\n\nfeedback discussion\n\n\nExercise 5: Create a GIThub account and start a discussion at the GIThub repository."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.1-overview.html#agenda",
    "href": "docs/developer/tutorial-data-management/slides/1.1-overview.html#agenda",
    "title": "1.1 Training Overview",
    "section": "Agenda",
    "text": "Agenda\n\n\n\nDay\nTopic\n\n\n\n\nDay 1\nFAIR principlesMetadata management\n\n\nDay 2\nMetadata publication\n\n\nDay 3\nData publicationQuality of Service\n\n\nDay 4\nCases\n\n\n\n\n\n\nWorkshop on spatial data flows"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#github",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#github",
    "title": "1.3 Tools",
    "section": "Github",
    "text": "Github\nService to facilitate co-creation\n\nversion history of contributions\nauthentication\nissue management\nrelease management\nContinuous integration\n\nRelated software; Git, Gitlab, Bitbucket, Codeberg"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#quarto",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#quarto",
    "title": "1.3 Tools",
    "section": "Quarto",
    "text": "Quarto\nA content management system to create websites, documentation, slides, etc.\nRelated software; Hugo, mkdocs, Jekyll, Wordpress, Drupal"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pycsw",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pycsw",
    "title": "1.3 Tools",
    "section": "pycsw",
    "text": "pycsw\nA catalogue server application providing support for multiple metadata schemes and metadata exchange standards\nRelated products; GeoNetwork, CKAN, Dataverse"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#mapserver",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#mapserver",
    "title": "1.3 Tools",
    "section": "Mapserver",
    "text": "Mapserver\nA server application providing OGC services (WMS, WFS, WCS) on various data formats\nRelated products; Geoserver, ArcGIS server, deegree, pygeoapi, QGIS server"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#leaflet",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#leaflet",
    "title": "1.3 Tools",
    "section": "Leaflet",
    "text": "Leaflet\nA javascript library to load interactive maps on a webpage.\n\nL = require('leaflet@1.9.4')\nhtml`&lt;link href='${resolve('leaflet@1.9.4/dist/leaflet.css')}' rel='stylesheet' /&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmap = {\n  let container = DOM.element('div', { style: `width:${width}px;height:${width/1.6}px` });\n  yield container;\n  let map = L.map(container).setView([-1.940278,29.873888], 13);\n  let osmLayer = L.tileLayer('https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}@2x.png', {\n      attribution: '&copy; &lt;a href=\"http://osm.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors'\n  }).addTo(map);\n  let nexrad = L.tileLayer.wms(\"https://maps.lsc-hubs.org/ows/clay\", {\n    layers: 'Clay_mean_0-20cm',\n    format: 'image/png',\n    opacity: 0.5,\n    transparent: 'true'\n}).addTo(map);;\n}\n\n\n\n\n\n\n\n\nRelated products; Openlayers, Cesium, Google Maps API, MapLibre"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#terriajs",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#terriajs",
    "title": "1.3 Tools",
    "section": "TerriaJS",
    "text": "TerriaJS\nA WebGIS application including Leaflet library\ndemo\nRelated products; GeoNode, GeoMoose, MapBender, Oskari, GisQuick"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#hale-desktop",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#hale-desktop",
    "title": "1.3 Tools",
    "section": "Hale Desktop",
    "text": "Hale Desktop\nSoftware to run Extract Transform and Load (ETL)\nRelated Software; FME, GDAL, Apache Hive,\n\n\n\nWorkshop on spatial data flows"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#what-is-ogc",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#what-is-ogc",
    "title": "3.1 OGC data services",
    "section": "What is OGC",
    "text": "What is OGC\n\nOpen Geospatial Consortium defines standards for the geospatial industry\nMembers are software companies, universities and govenment\nSuch as WMS, GML, OGCAPI, EPSG, GeoSPARQL\nRead more at https://opengeospatial.org"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "title": "3.1 OGC data services",
    "section": "OWS vs OGC API",
    "text": "OWS vs OGC API\n\nThe GML and OWS standards are defined on 20**\nSince then the internet moved away from xml\nin 2017 started spatial data on the web experiment with W3C, leading to OGC API\nCurrently under development\nBasic support in new and existing products\nThe browser as a tool to browse spatial data\n\n\n\n\nWorkshop on spatial data flows"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#history",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#history",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "History",
    "text": "History\nVarious efforts have been made to harmonise Soil Profile data\n\neSOTER\nGlobalSoilMap\nISO28258\nOGC Interoperability experiment\nGLOSIS Web ontology\nAnzSoil"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258",
    "text": "ISO28258\n\n\n\nISO28258 is based on Observations and measurements\nevery field observation and laboratory analysis as a sensor observation\n\n\n\n\n\nO&M model"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "GLOSIS Web Ontology",
    "text": "GLOSIS Web Ontology\n\nISO28258 expressed using common web ontologys (SOSA, VCARD)\nIncludes codelists for soil properties and procedures"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-1",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-1",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258",
    "text": "ISO28258\n\nA relational model is available according to ISO28258\nEither as PostGreSQL or geopackage (sqlite)\nRead more at https://iso28258.isric.org\n\n\n\n\nWorkshop on spatial data flows"
  }
]